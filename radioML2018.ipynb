{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vg_9Lbc6V8XC",
        "D-hlPXrPeU9f",
        "R2KJtkyG-0oc",
        "q0xhsIWz-AYg",
        "vqc7r9nA8asL",
        "5_oqKh2rgonc",
        "348KwxC-JfmX",
        "U1ezlD0rqgkL",
        "piO_gbgxrK3F",
        "xJ-9LpkYKEwH"
      ],
      "toc_visible": true,
      "mount_file_id": "1tXTwFaTpfx-Knnjup323pQfCIy6xUgeq",
      "authorship_tag": "ABX9TyNw4t0cFvCg1dMPRvqTwLYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eulerleonhard/Telecommunication_System_RadioML_HUST/blob/main/radioML2018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zad6iuE6ULe-"
      },
      "source": [
        "# Radio Modulation Recognition Networks\n",
        "\n",
        "---\n",
        "\n",
        "**Author: Ngân Hà**\n",
        "\n",
        "---\n",
        "\n",
        "**The code structure is following:**\n",
        "\n",
        "\n",
        "*   **Imports** - Import needed libraries\n",
        "*   **Defined Functions** - Functions defined for an easier manipulation with the data later on\n",
        "*   **Accessing the datasets** - you may skip this part and download the datasets elsewhere if you please\n",
        "*   **Loading Data** - Load the data and divide them into training, validation and test sets\n",
        "*   **Deep Learning Part** -Contains the architectures, which are prepared to be trained and evaluated\n",
        "*   **Load Trained Model** - Optionaly you can download the CGDNN model and see how it does on the corresponding dataset\n",
        "*   **Layer Visualization** - A part of code which was written to visualize the activation maps of the convolutional and recurrent layers\n",
        "*   **Plotting** - You can plot the confusion matrices in this part\n",
        "\n",
        "---\n",
        "\n",
        "**Quick guide to running the document:**\n",
        "\n",
        "*   Use `up` and `down` keys to move in the notebook\n",
        "*   Use `ctrl+enter` to run cell or choose 'Run All' in Runtime to run the whole document at once\n",
        "*   If you change something in specific cell, it's enough to re-run just the cell to save the changes\n",
        "*   Hide/show sections of the code with the arrows at side, which are next to some cell code\n",
        "* In the top left part you can click on the Content icon, which will allow you to navigate easier through this notebook\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVF306fgdgEB",
        "outputId": "37fca137-be32-439b-d4e2-5a83b661ec8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "#%cd '/content/drive/MyDrive/Colab Notebooks/BTL HTVT/RadioML2018/data'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oa7JTqi3TxUE",
        "outputId": "f9fc1a67-89f2-42a2-b978-6c7d04a7e60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/home/fil-server'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crawl [radioML2018.01A dataset](https://www.kaggle.com/aliayub/radioml2018) from Kaggle"
      ],
      "metadata": {
        "id": "vg_9Lbc6V8XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Kaggle enviroment\n",
        "\n",
        "### Ensure that you've already upload the kaggle.json file to /content"
      ],
      "metadata": {
        "id": "D-hlPXrPeU9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Ensure that you've already upload the kaggle.json file to /content\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "dMKWx3L_lNoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82325f1-9aff-41dd-a80b-e1fea6378d0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting kaggle\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "     |████████████████████████████████| 58 kB 401 kB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: python-dateutil in /usr/lib/python3/dist-packages (from kaggle) (2.7.3)\n",
            "Collecting python-slugify\n",
            "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kaggle) (2.22.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "     |████████████████████████████████| 76 kB 1.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle) (1.25.8)\n",
            "Collecting text-unidecode>=1.3\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "     |████████████████████████████████| 78 kB 1.4 MB/s             \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73052 sha256=31dcceb5e8b58133bef2ec3e7e04afce7553035e706e742ee2698ffb568dc8e4\n",
            "  Stored in directory: /home/fil-server/.cache/pip/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n",
            "Successfully built kaggle\n",
            "Installing collected packages: text-unidecode, tqdm, python-slugify, kaggle\n",
            "Successfully installed kaggle-1.5.12 python-slugify-5.0.2 text-unidecode-1.3 tqdm-4.62.3\n",
            "mkdir: cannot create directory ‘/home/fil-server/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'radioML2018'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7BqbN_xVPXP",
        "outputId": "89dd9181-8670-4787-8a65-9ebe8af108df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/fil-server/radioML2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download pinxau1000/radioml2018"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XuKv1OUlsvp",
        "outputId": "e905dc36-e19d-4f1f-d065-885e4be205ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading radioml2018.zip to /home/fil-server/radioML2018\n",
            "100%|████████████████████████████████████| 18.0G/18.0G [2:14:05<00:00, 2.43MB/s]\n",
            "100%|████████████████████████████████████| 18.0G/18.0G [2:14:05<00:00, 2.40MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip '/home/fil-server/radioML2018/radioml2018.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP8J32pArObp",
        "outputId": "e822429a-9aca-461e-e463-d9f8e07d003f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /home/fil-server/radioML2018/radioml2018.zip\n",
            "  inflating: GOLD_XYZ_OSC.0001_1024.hdf5  \n",
            "  inflating: LICENSE.TXT             \n",
            "  inflating: classes-fixed.json      \n",
            "  inflating: classes-fixed.txt       \n",
            "  inflating: classes.txt             \n",
            "  inflating: datasets.desktop        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize data example"
      ],
      "metadata": {
        "id": "4zFUvvWfocVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py\n",
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vk2iSsRNH8bo",
        "outputId": "a80dd500-9196-47c8-ea4a-1441b2cd18e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.3.1)\n",
            "Requirement already satisfied: h5py in /home/fil-server/.local/lib/python3.8/site-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /home/fil-server/.local/lib/python3.8/site-packages (from h5py) (1.22.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "     |████████████████████████████████| 11.3 MB 2.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (7.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from matplotlib) (20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/fil-server/.local/lib/python3.8/site-packages (from matplotlib) (1.22.1)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.29.0-py3-none-any.whl (895 kB)\n",
            "     |████████████████████████████████| 895 kB 2.7 MB/s            \n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "     |████████████████████████████████| 1.2 MB 3.3 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3/dist-packages (from matplotlib) (2.7.3)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Installing collected packages: kiwisolver, fonttools, cycler, matplotlib\n",
            "Successfully installed cycler-0.11.0 fonttools-4.29.0 kiwisolver-1.3.2 matplotlib-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import json\n",
        "from numpy import argwhere\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Open the dataset\n",
        "hdf5_file = h5py.File(\"GOLD_XYZ_OSC.0001_1024.hdf5\",  'r')\n",
        "# Load the modulation classes. You can also copy and paste the content of classes-fixed.txt.\n",
        "modulation_classes = json.load(open(\"classes-fixed.json\", 'r'))\n",
        "\n",
        "# Read the HDF5 groups\n",
        "data = hdf5_file['X']\n",
        "modulation_onehot = hdf5_file['Y']\n",
        "snr = hdf5_file['Z']\n",
        "\n",
        "# Sets the frame number\n",
        "idx = 235711\n",
        "\n",
        "# Converts the onehot encoded modulation to a readable string\n",
        "modulation_str = modulation_classes[int(argwhere(modulation_onehot[idx] == 1))]\n",
        "\n",
        "# Prints info about the frame\n",
        "print(f\"Retrieving Sample {idx}\\n\"\n",
        "      f\"\\t- Modulation (raw): {modulation_onehot[idx]}\\n\"\n",
        "      f\"\\t- Modulation: {modulation_str}\\n\"\n",
        "      f\"\\t- SNR: {snr[idx]}\\n\"\n",
        "      f\"\\t- Samples: {data[idx]}\")\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (9,4)\n",
        "\n",
        "for i in [0]:\n",
        "    plt.plot(signals[i,0,:],label=r'I')\n",
        "    plt.plot(signals[i,1,:],label=r'Q')\n",
        "    plt.xlabel(r'Sample')\n",
        "    plt.title( classes[modulations[i]].decode(\"utf-8\") )\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot IQ samples of the frame\n",
        "plt.figure()\n",
        "plt.title(f\"{modulation_str} with {snr[idx]}dB\")\n",
        "plt.plot(data[idx])\n",
        "plt.show()\n",
        "\n",
        "# Closes the file\n",
        "#hdf5_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "YLx_ilYOonVE",
        "outputId": "d6b25f73-0f13-4fd2-8a38-f07b5c6f89fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving Sample 235711\n",
            "\t- Modulation (raw): [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\t- Modulation: 8ASK\n",
            "\t- SNR: [-10]\n",
            "\t- Samples: [[ 0.41141438  0.0955532 ]\n",
            " [-0.02774137  0.16980176]\n",
            " [ 0.11000113  0.28186926]\n",
            " ...\n",
            " [ 0.97717977  0.6372246 ]\n",
            " [ 0.6477459  -1.0259153 ]\n",
            " [ 0.38088095 -0.77991676]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Show loss curves\n",
        "def show_history(history):\n",
        "    plt.figure()\n",
        "    plt.title('Training loss performance')\n",
        "    plt.plot(history.epoch, history.history['loss'], label='train loss+error')\n",
        "    plt.plot(history.epoch, history.history['val_loss'], label='val_error')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title('Training accuracy performance')\n",
        "    plt.plot(history.epoch, history.history['acc'], label='train_acc')\n",
        "    plt.plot(history.epoch, history.history['val_acc'], label='val_acc')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[],save_filename=None):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(labels))\n",
        "    plt.xticks(tick_marks, labels, rotation=45)\n",
        "    plt.yticks(tick_marks, labels)\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    if save_filename is not None:\n",
        "        plt.savefig(save_filename)\n",
        "    plt.show()\n",
        "\n",
        "def calculate_confusion_matrix(Y,Y_hat,classes):\n",
        "    n_classes = len(classes)\n",
        "    conf = np.zeros([n_classes,n_classes])\n",
        "    confnorm = np.zeros([n_classes,n_classes])\n",
        "\n",
        "    for k in range(0,Y.shape[0]):\n",
        "        i = list(Y[k,:]).index(1)\n",
        "        j = int(np.argmax(Y_hat[k,:]))\n",
        "        conf[i,j] = conf[i,j] + 1\n",
        "\n",
        "    for i in range(0,n_classes):\n",
        "        confnorm[i,:] = conf[i,:] / np.sum(conf[i,:])\n",
        "    # print(confnorm)\n",
        "\n",
        "    right = np.sum(np.diag(conf))\n",
        "    wrong = np.sum(conf) - right\n",
        "    return confnorm,right,wrong\n",
        "\n",
        "def calculate_accuracy_each_snr(Y,Y_hat,Z,classes=None):\n",
        "    Z_array = Z[:,0]\n",
        "    snrs = sorted(list(set(Z_array)))\n",
        "    # snrs = np.arange(-20,32,2)\n",
        "    acc = np.zeros(len(snrs))\n",
        "\n",
        "    Y_index = np.argmax(Y,axis=1)\n",
        "    Y_index_hat = np.argmax(Y_hat,axis=1)\n",
        "\n",
        "    i = 0\n",
        "    for snr in snrs:\n",
        "        Y_snr = Y_index[np.where(Z_array == snr)]\n",
        "        Y_hat_snr = Y_index_hat[np.where(Z_array == snr)]\n",
        "\n",
        "        acc[i] = np.sum(Y_snr==Y_hat_snr)/Y_snr.shape[0]\n",
        "        i = i +1\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(snrs,acc, label='test_acc')\n",
        "    plt.xlabel(\"Signal to Noise Ratio\")\n",
        "    plt.ylabel(\"Classification Accuracy\")\n",
        "    plt.title(\"Classification Accuracy on RadioML 2018.01\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "'''\n",
        "Through the confusion matrix under a certain snr, calculate the acc of each type of signal under the snr, and return a column\n",
        "'''\n",
        "def calculate_acc_at1snr_from_cm(cm):\n",
        "    return np.round(np.diag(cm)/np.sum(cm,axis=1),3)\n",
        "\n",
        "def calculate_acc_cm_each_snr(Y,Y_hat,Z,classes=None,save_figure=True,min_snr = 0):\n",
        "    Z_array = Z[:,0]\n",
        "    snrs = sorted(list(set(Z_array)))\n",
        "    acc = np.zeros(len(snrs))\n",
        "\n",
        "    acc_mod_snr = np.zeros( (len(classes),len(snrs)) )  #mods*snrs,24*26\n",
        "    i = 0\n",
        "    for snr in snrs:\n",
        "        Y_snr = Y[np.where(Z_array == snr)]\n",
        "        Y_hat_snr = Y_hat[np.where(Z_array == snr)]\n",
        "\n",
        "        # plot confusion for each snr\n",
        "        cm,right,wrong = calculate_confusion_matrix(Y_snr,Y_hat_snr,classes)\n",
        "        print(min_snr)\n",
        "        if snr >= min_snr:\n",
        "            plot_confusion_matrix(cm, title='Confusion matrix at {}db'.format(snr), cmap=plt.cm.Blues, labels=classes,save_filename = 'figure/cm_snr{}.png'.format(snr))\n",
        "\n",
        "        # cal acc on each snr\n",
        "        acc[i] = round(1.0*right/(right+wrong),3)\n",
        "        print('Accuracy at %ddb:%.2f%s / (%d + %d)' % (snr,100*acc[i],'%',right, wrong))\n",
        "\n",
        "        acc_mod_snr[:,i] = calculate_acc_at1snr_from_cm(cm)\n",
        "\n",
        "        i = i +1\n",
        "\n",
        "    '''\n",
        "    The change curve of acc with snr\n",
        "    '''\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(snrs,acc, label='test_acc')\n",
        "    # Set number labels\n",
        "    for x, y in zip(snrs,acc):\n",
        "        plt.text(x, y, y, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "    plt.xlabel(\"Signal to Noise Ratio\")\n",
        "    plt.ylabel(\"Classification Accuracy\")\n",
        "    plt.title(\"Classification Accuracy on All Test Data\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig('figure/acc_overall.png')\n",
        "    plt.show()\n",
        "\n",
        "    fd = open('acc_overall_128k_on_512k_wts.dat', 'wb')\n",
        "    pickle.dump(('128k','512k', acc), fd)\n",
        "    fd.close()\n",
        "\n",
        "    '''\n",
        "    acc changes curve with snr, one curve per mod\n",
        "    '''\n",
        "    dis_num = 6\n",
        "    for g in range(int(np.ceil(acc_mod_snr.shape[0]/dis_num))):\n",
        "        assert (0 <= dis_num <= acc_mod_snr.shape[0])\n",
        "        beg_index = g*dis_num\n",
        "        end_index = np.min([(g+1)*dis_num,acc_mod_snr.shape[0]])\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        plt.xlabel(\"Signal to Noise Ratio\")\n",
        "        plt.ylabel(\"Classification Accuracy\")\n",
        "        plt.title(\"Classification Accuracy for Each Mod\")\n",
        "\n",
        "        for i in range(beg_index,end_index):\n",
        "            plt.plot(snrs, acc_mod_snr[i], label=classes[i])\n",
        "            # Set number labels\n",
        "            for x, y in zip(snrs, acc_mod_snr[i]):\n",
        "                plt.text(x, y, y, ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        if save_figure:\n",
        "            plt.savefig('figure/acc_with_mod_{}.png'.format(g+1))\n",
        "        plt.show()\n",
        "\n",
        "    fd = open('acc_for_mod_on_1m_wts.dat', 'wb')\n",
        "    pickle.dump(('128k','1m', acc_mod_snr), fd)\n",
        "    fd.close()\n",
        "    # print(acc_mod_snr)\n",
        "\n",
        "0"
      ],
      "metadata": {
        "id": "uofgL2EKu4qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preambule**"
      ],
      "metadata": {
        "id": "R2KJtkyG-0oc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing\n"
      ],
      "metadata": {
        "id": "q0xhsIWz-AYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA_LgBvmJB_S",
        "outputId": "84866f8a-bbc9-484d-f15f-3b4247a54809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.7.0-cp38-cp38-manylinux2010_x86_64.whl (489.6 MB)\n",
            "     |████████████████████████████████| 489.6 MB 25 kB/s              \n",
            "\u001b[?25hCollecting tensorboard~=2.6\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "     |████████████████████████████████| 5.8 MB 777 kB/s            \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "     |████████████████████████████████| 463 kB 2.0 MB/s            \n",
            "\u001b[?25hCollecting protobuf>=3.9.2\n",
            "  Downloading protobuf-3.19.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "     |████████████████████████████████| 1.1 MB 2.7 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /home/fil-server/.local/lib/python3.8/site-packages (from tensorflow) (3.6.0)\n",
            "Collecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.13.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (84 kB)\n",
            "     |████████████████████████████████| 84 kB 839 kB/s            \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "     |████████████████████████████████| 4.1 MB 3.0 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.32.0 in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "     |████████████████████████████████| 42 kB 362 kB/s            \n",
            "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/fil-server/.local/lib/python3.8/site-packages (from tensorflow) (4.0.1)\n",
            "Collecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "     |████████████████████████████████| 65 kB 2.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
            "Collecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "     |████████████████████████████████| 57 kB 915 kB/s            \n",
            "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "     |████████████████████████████████| 1.3 MB 3.5 MB/s            \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.21.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.23.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "     |████████████████████████████████| 2.1 MB 3.6 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /home/fil-server/.local/lib/python3.8/site-packages (from tensorflow) (1.22.1)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hCollecting absl-py>=0.4.0\n",
            "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
            "     |████████████████████████████████| 126 kB 3.6 MB/s            \n",
            "\u001b[?25hCollecting libclang>=9.0.1\n",
            "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
            "     |████████████████████████████████| 14.5 MB 2.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (45.2.0)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "     |████████████████████████████████| 781 kB 2.7 MB/s            \n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.5.0-py2.py3-none-any.whl (157 kB)\n",
            "     |████████████████████████████████| 157 kB 3.2 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow) (2.22.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "     |████████████████████████████████| 4.9 MB 2.9 MB/s            \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
            "     |████████████████████████████████| 288 kB 2.7 MB/s            \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "     |████████████████████████████████| 155 kB 3.6 MB/s            \n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "     |████████████████████████████████| 77 kB 1.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.0)\n",
            "Building wheels for collected packages: termcolor\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=eecfdb37d2cfb1c13f9fdd1bd2d1647d6f8e03b9469247249e81980909ea9d82\n",
            "  Stored in directory: /home/fil-server/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "Successfully built termcolor\n",
            "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
            "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.4.0 google-auth-2.5.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.43.0 keras-2.7.0 keras-preprocessing-1.1.2 libclang-13.0.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-io-gcs-filesystem-0.23.1 termcolor-1.1.0 werkzeug-2.0.2 wrapt-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from os import mkdir\n",
        "from os.path import exists as path_exists\n",
        "from os.path import join\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['pdf.fonttype'] = 42\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow import convert_to_tensor\n",
        "from tensorflow.keras.backend import clear_session\n",
        "\n",
        "print(f'Available devices:\\n{[x.name for x in device_lib.list_local_devices()]}')\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, AlphaDropout, GlobalAveragePooling1D, BatchNormalization, add\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Reshape, Activation\n",
        "from tensorflow.keras.layers import Conv1D, AveragePooling1D, ZeroPadding2D, Convolution2D, MaxPooling1D, MaxPooling2D\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-P7PhKH-C-F",
        "outputId": "e7c1eea6-c688-4064-bc7b-5ff7078bac2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-29 21:05:54.792396: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2022-01-29 21:05:54.792419: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available devices:\n",
            "['/device:CPU:0']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-29 21:05:56.545091: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-01-29 21:05:56.546737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-01-29 21:05:56.546758: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-01-29 21:05:56.546776: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fil-server-08): /proc/driver/nvidia/version does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for lib in ['tensorflow','numpy','matplotlib']:\n",
        "    print(f'{lib} version = {__import__(lib).__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unCPezBy-Jsx",
        "outputId": "7c8de622-b3b5-48af-8ec2-881ffb63c473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow version = 2.7.0\n",
            "numpy version = 1.22.1\n",
            "matplotlib version = 3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2022)"
      ],
      "metadata": {
        "id": "wYhFxXQv-SOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General settings"
      ],
      "metadata": {
        "id": "xIYdxORl--7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please specify the folder where your dataset(s) is/are\n",
        "data_path = '/home/fil-server/radioML2018' # <your data path>\n",
        "\n",
        "# Please specify a folder to log information during model training\n",
        "log_path = '/home/fil-server/radioML2018' # <your log path>\n",
        "\n",
        "if not path_exists(log_path):\n",
        "    mkdir(log_path)"
      ],
      "metadata": {
        "id": "Q1UpRcm4_EGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset settings and Training settings"
      ],
      "metadata": {
        "id": "5IkdhRet_eM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset setting\n",
        "dataset_name = 'radioML2018.01A'\n",
        "signal_duration = 1024\n",
        "snr_cut = 0\n",
        "\n",
        "# Training setting\n",
        "nb_epoch = 200\n",
        "batch_size = 512"
      ],
      "metadata": {
        "id": "BrNChkyU6Ony"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data utils**"
      ],
      "metadata": {
        "id": "vqc7r9nA8asL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from numpy.random import choice\n",
        "\n",
        "class TimeHistory(Callback):\n",
        "    \"\"\" Keras callback tto monitor execution time during training\n",
        "    \"\"\"\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)\n",
        "\n",
        "def split_dataset( data, labels,\n",
        "                    p_train=0.8, p_valid=0.0, p_test=0.0,\n",
        "                    ):\n",
        "    \"\"\" Function to split and shuffle a dataset into train, validation and test\n",
        "        splits.\n",
        "        Arguments:\n",
        "        data (tensor): input dataset\n",
        "        labels (tensor): label vector associated to data\n",
        "        p_train (float): proportion of training data\n",
        "        p_valid (float): proportion of validation data\n",
        "        p_test (float): proportion of test data\n",
        "    \"\"\"\n",
        "\n",
        "    def aux(D_ , L_,  p_, idx_):\n",
        "\n",
        "        n_tot_examples_ = D_.shape[0]\n",
        "        n_taken_examples_ = int(n_tot_examples_ * p_ )\n",
        "        chosen_idx_ = choice(idx_ , size=n_taken_examples_, replace=False)\n",
        "        free_indexes_ = list( set(idx_) - set( chosen_idx_ ) )\n",
        "\n",
        "        return D_[chosen_idx_] , L_[chosen_idx_] , chosen_idx_, free_indexes_\n",
        "\n",
        "    D = data\n",
        "    L = labels\n",
        "\n",
        "    idx = range(0, D.shape[0])\n",
        "    X_train , Y_train, train_idx, free_idx = aux(D , L, p_train ,  idx)\n",
        "\n",
        "    idx = free_idx\n",
        "    X_valid , Y_valid, valid_idx, free_idx = aux(D , L, p_valid ,  idx)\n",
        "\n",
        "    idx = free_idx\n",
        "    X_test, Y_test, test_idx,free_idx = aux(D , L, p_test ,  idx)\n",
        "\n",
        "    return X_train , Y_train , train_idx, X_valid , Y_valid, valid_idx, X_test , Y_test,test_idx\n",
        "\n",
        "    pass"
      ],
      "metadata": {
        "id": "knCwbLi68bGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data loading and preparation**"
      ],
      "metadata": {
        "id": "5_oqKh2rgonc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the selected dataset ;\n",
        "\n",
        "This cell outputs:\n",
        "\n",
        "`signals` : a numpy array containing signal data\n",
        "\n",
        "`class_onehot` : a numpy array containing one hot encoded labels associated to data\n",
        "\n",
        "`snrs`: an array associating snr to each signal\n",
        "\n",
        "`class_list`: a list of strings associating the modulation / signal kind name to the indexes of labels"
      ],
      "metadata": {
        "id": "UbZEjKO86pHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from h5py import File\n",
        "from numpy import array, zeros, sqrt, mean, vstack\n",
        "from numpy.random import choice\n",
        "import pickle\n",
        "\n",
        "def read_RML2018(fName ,nb_examples = None):\n",
        "    \"\"\" Open datasets from Radio Machine Learning 2018\n",
        "    Args:\n",
        "        fname (str, optional): [description]. Defaults to './2016.04C.multisnr.pkl'.\n",
        "        snrs (list of int, optional): list of snrs to keep. If None keeps all.  Defaults to None.\n",
        "        verbose (bool): set verbosity\n",
        "    Returns:\n",
        "        (tuple of arrays): data, labels, snrs, None\n",
        "    \"\"\"\n",
        "    with File(fName,'r') as h:\n",
        "\n",
        "        if not nb_examples is None:\n",
        "            w = choice(h['X'].shape[0], size=nb_examples, replace=False)\n",
        "            data = h['X'][:][w]\n",
        "            mods_ = h['Y'][:][w]\n",
        "            snrs_ = h['Z'][:][w]\n",
        "        else:\n",
        "            data = h['X'][:]\n",
        "            mods_ = h['Y'][:]\n",
        "            snrs_ = h['Z'][:]\n",
        "\n",
        "    snrs_ = snrs_.flatten()\n",
        "    snrs_=snrs_.reshape(-1)\n",
        "    data = data.transpose((0,2,1))\n",
        "\n",
        "    return data, mods_, snrs_,None"
      ],
      "metadata": {
        "id": "0kCUpvAPh4vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "signals, class_onehot, snrs ,class_list = read_RML2018('/home/fil-server/radioML2018/GOLD_XYZ_OSC.0001_1024.hdf5')\n",
        "class_idx = np.argmax(class_onehot,axis=-1)"
      ],
      "metadata": {
        "id": "rdtkfF5K2iQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snrs_list = sorted( list( set( snrs ) ) )\n",
        "\n",
        "print(f'{dataset_name} loaded, {signals.shape[0]} signal with shape {signals.shape[1:]}' )\n",
        "print(f'List of signal SNR: {snrs_list}')\n",
        "print(f'List of modulations under consideration: {class_list}')"
      ],
      "metadata": {
        "id": "Wa0r7YeH3o1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c50757-29d7-49a1-dc80-6b228c0ae142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "radioML2018.01A loaded, 2555904 signal with shape (2, 1024)\n",
            "List of signal SNR: [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]\n",
            "List of modulations under consideration: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transposes the data so the real/imag axis is the last as expected by deep learning models\n"
      ],
      "metadata": {
        "id": "QdwboTiuAmrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Initial data shape: {signals.shape}')\n",
        "signals = signals.transpose((0,2,1))\n",
        "print(f'Transposed data shape: {signals.shape}' )"
      ],
      "metadata": {
        "id": "HKvwrCapAkGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336c6f27-7df6-499a-c99b-67705d60b981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (2555904, 2, 1024)\n",
            "Transposed data shape: (2555904, 1024, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trunk signal to `signal_duration` samples."
      ],
      "metadata": {
        "id": "i5SL-2JGAt9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not signal_duration is None:\n",
        "    print(f'Initial data shape: {signals.shape}')\n",
        "    signals = signals[:,:signal_duration,:]\n",
        "    print(f'Trunked data shape: {signals.shape}' )\n"
      ],
      "metadata": {
        "id": "lePkTLxs7Iyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd31971-5042-478e-c73f-475ae812aec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (2555904, 1024, 2)\n",
            "Trunked data shape: (2555904, 1024, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filters signal by snr if requested above"
      ],
      "metadata": {
        "id": "vhIspTuEA8J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not snr_cut is None:\n",
        "    print(f'Initial data shape: {signals.shape}')\n",
        "    w = snrs>=snr_cut\n",
        "    signals = signals[w]\n",
        "    class_idx = class_idx[w]\n",
        "    snrs = snrs[w]\n",
        "    class_onehot = class_onehot[w]\n",
        "    print(f'New data shape: {signals.shape}' )"
      ],
      "metadata": {
        "id": "aqsojbQRA-0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314ab206-012d-4917-f86d-8acf91010ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (2555904, 1024, 2)\n",
            "New data shape: (1572864, 1024, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the power of each signal"
      ],
      "metadata": {
        "id": "A_Nh4HRa7uZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Normalize the power of each signal\n",
        "norm = np.sqrt(np.mean(signals**2,axis=(1,2),keepdims=True))\n",
        "signals/=norm"
      ],
      "metadata": {
        "id": "9opVvV9t7Mg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset into train and test sets. Also shuffles data."
      ],
      "metadata": {
        "id": "F9fcppzp75oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, train_idx ,_,_,_, X_test,y_test,test_idx= split_dataset(signals,\n",
        "    class_onehot,\n",
        "    p_train=0.5,p_test=0.5)\n",
        "\n",
        "SNR_train = snrs[train_idx]\n",
        "SNR_test = snrs[test_idx]\n",
        "\n",
        "print(f'Train dataset shape: {X_train.shape}')\n",
        "print(f'Test dataset shape: {X_test.shape} ')\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'X_test  shape: {X_test.shape}, y_test  shape: {y_test.shape}')\n",
        "\n",
        "input_shp = list(X_train.shape[1:])\n",
        "output_shp = y_train.shape[1]\n",
        "\n",
        "print(f'Network input shape in variable input-shp: {input_shp}')\n",
        "print(f'Network output shape in variable input-shp: {output_shp}')\n",
        "\n",
        "\n",
        "X_train = convert_to_tensor(X_train)\n",
        "y_train = convert_to_tensor(y_train)\n",
        "X_test = convert_to_tensor(X_test)\n",
        "y_test = convert_to_tensor(y_test)"
      ],
      "metadata": {
        "id": "PY2XaM6472cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3c246d-e574-4ce4-e129-93efa40c6c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset shape: (786432, 1024, 2)\n",
            "Test dataset shape: (786432, 1024, 2) \n",
            "X_train shape: (786432, 1024, 2), y_train shape: (786432, 24)\n",
            "X_test  shape: (786432, 1024, 2), y_test  shape: (786432, 24)\n",
            "Network input shape in variable input-shp: [1024, 2]\n",
            "Network output shape in variable input-shp: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-05 16:37:03.472170: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 6442450944 exceeds 10% of free system memory.\n",
            "2022-02-05 16:37:07.565731: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 6442450944 exceeds 10% of free system memory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep learning model training**"
      ],
      "metadata": {
        "id": "3rKe0LFyB1bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to instanciate and train a deep learning model"
      ],
      "metadata": {
        "id": "6nQj6aggB8Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,name_network):\n",
        "\n",
        "    clear_session()\n",
        "    time_callback = TimeHistory()\n",
        "\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        epochs=nb_epoch, batch_size=batch_size,\n",
        "                        validation_data =( X_test , y_test ),\n",
        "                        verbose=1,\n",
        "                        callbacks=[time_callback])\n",
        "\n",
        "    model.save(join(log_path,'model-{}-{}-trained{}.h5'.format(dataset_name,name_network,signal_duration)))\n",
        "\n",
        "    data = np.array(list(zip(history.epoch, history.history['val_accuracy'], history.history['accuracy'])))\n",
        "    np.savetxt(join(log_path,'history-{}-{}-trained{}.txt'.format(dataset_name,name_network,signal_duration)),data)\n",
        "\n",
        "    data = np.array(list(zip(history.epoch, time_callback.times)))\n",
        "    np.savetxt(join(log_path,'history_time-{}-{}-trained{}.txt'.format(dataset_name,name_network,signal_duration)),data)\n",
        "\n",
        "    clear_session()"
      ],
      "metadata": {
        "id": "rF1fKMI19cir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Structures"
      ],
      "metadata": {
        "id": "348KwxC-JfmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def residual_stack(X,Filters,Seq,max_pool):\n",
        "      \"\"\" Auxiliary function to generate RML CNN/VGG  as defined in O'Shea et Al., Over-the-Air Deep Learning\n",
        "          Based Radio Signal Classification,  2018. The implementation is an adaptation of\n",
        "          https://github.com/liuzhejun/ResNet-for-Radio-Recognition/blob/master/ResNet_Model.ipynb\n",
        "      Arguments:\n",
        "          X (tensor): input\n",
        "          Filters (int): number of filters\n",
        "          Seq (str): module name\n",
        "          max_pool (bool): enable max pooling at the end\n",
        "      \"\"\"\n",
        "\n",
        "      #1*1 Conv Linear\n",
        "      X = Convolution2D(Filters, (1, 1), padding='same',\n",
        "                        name=Seq+\"_conv1\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      #Residual Unit 1\n",
        "      X_shortcut = X\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',activation=\"relu\",\n",
        "                        name=Seq+\"_conv2\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',\n",
        "                        name=Seq+\"_conv3\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      #Residual Unit 2\n",
        "      X_shortcut = X\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',activation=\"relu\",\n",
        "                        name=Seq+\"_conv4\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',\n",
        "                        name=Seq+\"_conv5\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      #MaxPooling\n",
        "      if max_pool:\n",
        "          X = MaxPooling2D(pool_size=(2, 1), strides=(2, 1), padding='valid',\n",
        "                          data_format=\"channels_last\")(X)\n",
        "\n",
        "      return X\n"
      ],
      "metadata": {
        "id": "WWNslqeQsCC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class neural_nets_keras:\n",
        "  def get_LModCNN(input_shp,output_shp,verbose=False):\n",
        "      \"\"\" Generate LModCNN architecture as defined in Courtat and du Mas des Bourboux,\n",
        "          A light neural network for modulation detection under impairments, ISNCC 2021\n",
        "      Arguments:\n",
        "          input_shp (list): shape of the input data [signal_length,2], batch is omitted\n",
        "          output_shp (list): shape of the output data [n_classes]\n",
        "          verbose (bool): set verbosity\n",
        "      \"\"\"\n",
        "      model = Sequential()\n",
        "\n",
        "      model.add(Conv1D(filters=8, kernel_size=7, activation='relu', padding=\"same\",input_shape=input_shp))\n",
        "\n",
        "      model.add(Conv1D(filters=16, kernel_size=7, activation='relu', padding=\"same\"))\n",
        "\n",
        "      model.add(Conv1D(filters=32, kernel_size=7, activation='relu', padding=\"same\"))\n",
        "\n",
        "      model.add(Conv1D(filters=64, kernel_size=7, activation='relu', padding=\"same\"))\n",
        "\n",
        "      model.add(GlobalAveragePooling1D())\n",
        "\n",
        "      model.add(Dense(units=256, activation='relu', kernel_initializer='he_normal'))\n",
        "      model.add(Dropout(rate=0.5))\n",
        "\n",
        "      model.add(Dense(output_shp, activation='softmax', kernel_initializer='he_normal'))\n",
        "\n",
        "      if verbose:\n",
        "          model.summary()\n",
        "\n",
        "      return model\n",
        "\n",
        "  def get_LModCNNResNetRelu(input_shp,output_shp,verbose=False):\n",
        "      \"\"\" Generate LMod CNN with residual connexion architecture as defined in Courtat and du Mas des Bourboux,\n",
        "          A light neural network for modulation detection under impairments, ISNCC 2021\n",
        "      Arguments:\n",
        "          input_shp (list): shape of the input data [signal_length,2], batch is omitted\n",
        "          output_shp (list): shape of the output data [n_classes]\n",
        "          verbose (bool): set verbosity\n",
        "      \"\"\"\n",
        "      kernel_size = 7\n",
        "\n",
        "      X_input = Input(input_shp)\n",
        "\n",
        "      X = Conv1D(filters=8, kernel_size=1, activation='relu', padding=\"same\",\n",
        "                input_shape=input_shp,\n",
        "                kernel_initializer='glorot_uniform')(X_input)\n",
        "\n",
        "      X_shortcut = X\n",
        "\n",
        "      X = Conv1D(filters=8, kernel_size=kernel_size, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = Conv1D(filters=8, kernel_size=kernel_size, padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      X = Conv1D(filters=16, kernel_size=1, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "      X_shortcut = X\n",
        "\n",
        "      X = Conv1D(filters=16, kernel_size=kernel_size, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = Conv1D(filters=16, kernel_size=kernel_size, padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      X = Conv1D(filters=32, kernel_size=1, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "      X_shortcut = X\n",
        "\n",
        "      X = Conv1D(filters=32, kernel_size=kernel_size, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = Conv1D(filters=32, kernel_size=kernel_size, padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      X = Conv1D(filters=64, kernel_size=1, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "\n",
        "      X_shortcut = X\n",
        "\n",
        "      X = Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = Conv1D(filters=64, kernel_size=kernel_size, padding=\"same\",\n",
        "                kernel_initializer='glorot_uniform')(X)\n",
        "      X = add([X,X_shortcut])\n",
        "\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      X = GlobalAveragePooling1D()(X)\n",
        "\n",
        "      X = Dense(units=256, activation='relu', kernel_initializer='he_normal')(X)\n",
        "      X = Dropout(rate=0.5)(X)\n",
        "\n",
        "      X = Dense(output_shp, activation='softmax', kernel_initializer='he_normal')(X)\n",
        "\n",
        "      model = Model(inputs=X_input,outputs=X)\n",
        "\n",
        "      if verbose:\n",
        "          model.summary()\n",
        "\n",
        "      return model\n",
        "\n",
        "  def get_RMLConvNet(input_shp,output_shp,verbose=False):\n",
        "      \"\"\" Generate RMLConvNet as defined in O'Shea et Al., Convolutional radio\n",
        "      modulation recognition networks, 2016\n",
        "      the implementation is an adaptation of\n",
        "      https://github.com/radioML/examples/blob/master/modulation_recognition/RML2016.10a_VTCNN2_example.ipynb\n",
        "      Arguments:\n",
        "      input_shp (list): shape of the input data [signal_length,2], batch is omitted\n",
        "      output_shp (list): shape of the output data [n_classes]\n",
        "      verbose (bool): set verbosity\n",
        "      \"\"\"\n",
        "\n",
        "      dr = 0.5\n",
        "\n",
        "      model = Sequential()\n",
        "      model.add(Reshape(input_shp+[1], input_shape=input_shp))\n",
        "\n",
        "      model.add(ZeroPadding2D((2, 0), data_format='channels_last'))\n",
        "      model.add(Convolution2D(256, (3, 1), padding='valid', activation=\"relu\", kernel_initializer='glorot_uniform', data_format='channels_last'))\n",
        "      model.add(Dropout(dr))\n",
        "\n",
        "      model.add(ZeroPadding2D((2, 0),data_format='channels_last'))\n",
        "      model.add(Convolution2D(80, (3, 2), padding=\"valid\", activation=\"relu\", kernel_initializer='glorot_uniform', data_format='channels_last'))\n",
        "      model.add(Dropout(dr))\n",
        "\n",
        "      model.add(Flatten())\n",
        "      model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
        "      model.add(Dropout(dr))\n",
        "      model.add(Dense(output_shp, kernel_initializer='he_normal', activation='softmax'))\n",
        "\n",
        "      if verbose:\n",
        "          model.summary()\n",
        "\n",
        "      return model\n",
        "\n",
        "  def get_RMLCNNVGG(input_shp,output_shp,verbose=False):\n",
        "      \"\"\" Generate RML CNN/VGG  as defined in O'Shea et Al., Over-the-Air Deep Learning\n",
        "          Based Radio Signal Classification,  2018. The implementation is an adaptation of\n",
        "          https://github.com/leena201818/radioml/blob/master/rmlmodels/VGGLikeModel.py\n",
        "      Arguments:\n",
        "          input_shp (list): shape of the input data [signal_length,2], batch is omitted\n",
        "          output_shp (list): shape of the output data [n_classes]\n",
        "          verbose (bool): set verbosity\n",
        "      \"\"\"\n",
        "\n",
        "      dr = 0.5\n",
        "      kernel_size = 7\n",
        "\n",
        "      model = Sequential()\n",
        "\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform',input_shape=input_shp))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform'))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform'))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform'))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform'))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform'))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "      model.add(Conv1D(filters=64, kernel_size=kernel_size, activation='relu', padding=\"same\", kernel_initializer='glorot_uniform'))\n",
        "      model.add(MaxPooling1D(strides=2))\n",
        "\n",
        "      model.add(Flatten())\n",
        "\n",
        "      model.add(Dense(128, activation='selu', kernel_initializer='he_normal'))\n",
        "      model.add(Dropout(dr))\n",
        "\n",
        "      model.add(Dense(128, activation='selu', kernel_initializer='he_normal'))\n",
        "      model.add(Dropout(dr))\n",
        "\n",
        "      model.add(Dense(output_shp, activation='softmax', kernel_initializer='he_normal'))\n",
        "\n",
        "      if verbose:\n",
        "          model.summary()\n",
        "\n",
        "      return model\n",
        "\n",
        "  def residual_stack(X,Filters,Seq,max_pool):\n",
        "      \"\"\" Auxiliary function to generate RML CNN/VGG  as defined in O'Shea et Al., Over-the-Air Deep Learning\n",
        "          Based Radio Signal Classification,  2018. The implementation is an adaptation of\n",
        "          https://github.com/liuzhejun/ResNet-for-Radio-Recognition/blob/master/ResNet_Model.ipynb\n",
        "      Arguments:\n",
        "          X (tensor): input\n",
        "          Filters (int): number of filters\n",
        "          Seq (str): module name\n",
        "          max_pool (bool): enable max pooling at the end\n",
        "      \"\"\"\n",
        "\n",
        "      #1*1 Conv Linear\n",
        "      X = Convolution2D(Filters, (1, 1), padding='same',\n",
        "                        name=Seq+\"_conv1\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      #Residual Unit 1\n",
        "      X_shortcut = X\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',activation=\"relu\",\n",
        "                        name=Seq+\"_conv2\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',\n",
        "                        name=Seq+\"_conv3\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      #Residual Unit 2\n",
        "      X_shortcut = X\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',activation=\"relu\",\n",
        "                        name=Seq+\"_conv4\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = Convolution2D(Filters, (3, 2), padding='same',\n",
        "                        name=Seq+\"_conv5\", kernel_initializer='glorot_uniform',\n",
        "                        data_format=\"channels_last\")(X)\n",
        "\n",
        "      X = add([X,X_shortcut])\n",
        "      X = Activation(\"relu\")(X)\n",
        "\n",
        "      #MaxPooling\n",
        "      if max_pool:\n",
        "          X = MaxPooling2D(pool_size=(2, 1), strides=(2, 1), padding='valid',\n",
        "                          data_format=\"channels_last\")(X)\n",
        "\n",
        "      return X\n",
        "\n",
        "  def get_RMLResNet(input_shp,output_shp,verbose=False):\n",
        "      \"\"\" Generate RML Residual Network  as defined in O'Shea et Al., Over-the-Air Deep Learning\n",
        "          Based Radio Signal Classification,  2018. The implementation is an adaptation of\n",
        "          https://github.com/liuzhejun/ResNet-for-Radio-Recognition/blob/master/ResNet_Model.ipynb\n",
        "      Arguments:\n",
        "          input_shp (list): shape of the input data [signal_length,2], batch is omitted\n",
        "          output_shp (list): shape of the output data [n_classes]\n",
        "          verbose (bool): set verbosity\n",
        "      \"\"\"\n",
        "\n",
        "      X_input = Input(input_shp)\n",
        "      X = Reshape(input_shp+[1], input_shape=input_shp)(X_input)\n",
        "      #Residual Srack 1\n",
        "      X = residual_stack(X,32,\"ReStk1\",False)  #shape:(1,512,32)\n",
        "      X = MaxPooling2D(pool_size=(2, 2), strides=(2, 1), padding='valid', data_format=\"channels_last\")(X)\n",
        "      #Residual Srack 2\n",
        "      X = residual_stack(X,32,\"ReStk2\",True)  #shape:(1,256,32)\n",
        "      #Residual Srack 3\n",
        "      X = residual_stack(X,32,\"ReStk3\",True)  #shape:(1,128,32)\n",
        "      #Residual Srack 4\n",
        "      X = residual_stack(X,32,\"ReStk4\",True)  #shape:(1,64,32)\n",
        "      #Residual Srack 5\n",
        "      X = residual_stack(X,32,\"ReStk5\",True)  #shape:(1,32,32)\n",
        "      #Residual Srack 6\n",
        "      X = residual_stack(X,32,\"ReStk6\",True)  #shape:(1,16,32)\n",
        "\n",
        "      #Full Con 1\n",
        "      X = Flatten()(X)\n",
        "      X = Dense(128, activation='selu', kernel_initializer='he_normal', name=\"dense1\")(X)\n",
        "      X = AlphaDropout(0.3)(X)\n",
        "      #Full Con 2\n",
        "      X = Dense(128, activation='selu', kernel_initializer='he_normal', name=\"dense2\")(X)\n",
        "      X = AlphaDropout(0.3)(X)\n",
        "      #Full Con 3\n",
        "      X = Dense(output_shp, kernel_initializer='he_normal', name=\"dense3\", activation='softmax')(X)\n",
        "\n",
        "      model = Model(inputs=X_input,outputs=X)\n",
        "\n",
        "      if verbose:\n",
        "          model.summary()\n",
        "\n",
        "      return model"
      ],
      "metadata": {
        "id": "HxsZNcddD4km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LModCNN\n",
        "\n",
        "Generate LModCNN architecture as defined in Courtat and du Mas des Bourboux, *A light neural network for modulation detection under impairments*, ISNCC 2021\n",
        "\n"
      ],
      "metadata": {
        "id": "U2fL-owqCn6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_network = \"LModCNN\"\n",
        "\n",
        "dynamic_input_shp = input_shp.copy()\n",
        "dynamic_input_shp[0] = None\n",
        "\n",
        "model = getattr(neural_nets_keras,'get_{}'.format(name_network))(dynamic_input_shp,output_shp)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sYHy-hXxC3GF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16270ca-708e-484b-d9d3-c144f8fb5e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, None, 8)           120       \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 16)          912       \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, None, 32)          3616      \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, None, 64)          14400     \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 64)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               16640     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                6168      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 41,856\n",
            "Trainable params: 41,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "train_model(model, name_network)"
      ],
      "metadata": {
        "id": "4iZqc5iOEPic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c62abf-6d04-4dcc-b875-68ccb2e7c03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1536/1536 [==============================] - 3155s 2s/step - loss: 0.2518 - accuracy: 0.8916 - val_loss: 0.2267 - val_accuracy: 0.9021\n",
            "Epoch 2/200\n",
            "1536/1536 [==============================] - 3159s 2s/step - loss: 0.2743 - accuracy: 0.8832 - val_loss: 0.2239 - val_accuracy: 0.9020\n",
            "Epoch 3/200\n",
            "1536/1536 [==============================] - 3134s 2s/step - loss: 0.2429 - accuracy: 0.8956 - val_loss: 0.2238 - val_accuracy: 0.9019\n",
            "Epoch 4/200\n",
            "1536/1536 [==============================] - 3133s 2s/step - loss: 0.2334 - accuracy: 0.8993 - val_loss: 0.2203 - val_accuracy: 0.9042\n",
            "Epoch 5/200\n",
            "1536/1536 [==============================] - 3135s 2s/step - loss: 0.2341 - accuracy: 0.8988 - val_loss: 0.2226 - val_accuracy: 0.9024\n",
            "Epoch 6/200\n",
            "1536/1536 [==============================] - 3133s 2s/step - loss: 0.2338 - accuracy: 0.8995 - val_loss: 0.2262 - val_accuracy: 0.9007\n",
            "Epoch 7/200\n",
            "1536/1536 [==============================] - 3137s 2s/step - loss: 0.2305 - accuracy: 0.9003 - val_loss: 0.2219 - val_accuracy: 0.9034\n",
            "Epoch 8/200\n",
            "1536/1536 [==============================] - 3134s 2s/step - loss: 0.2298 - accuracy: 0.9006 - val_loss: 0.2241 - val_accuracy: 0.9039\n",
            "Epoch 9/200\n",
            "1536/1536 [==============================] - 3146s 2s/step - loss: 0.2301 - accuracy: 0.9005 - val_loss: 0.2233 - val_accuracy: 0.9026\n",
            "Epoch 10/200\n",
            "1536/1536 [==============================] - 3145s 2s/step - loss: 0.2369 - accuracy: 0.8984 - val_loss: 0.2175 - val_accuracy: 0.9056\n",
            "Epoch 11/200\n",
            "1536/1536 [==============================] - 3138s 2s/step - loss: 0.2263 - accuracy: 0.9023 - val_loss: 0.2182 - val_accuracy: 0.9049\n",
            "Epoch 12/200\n",
            "1536/1536 [==============================] - 3136s 2s/step - loss: 0.2258 - accuracy: 0.9024 - val_loss: 0.2193 - val_accuracy: 0.9048\n",
            "Epoch 13/200\n",
            "1536/1536 [==============================] - 3136s 2s/step - loss: 0.2257 - accuracy: 0.9026 - val_loss: 0.2173 - val_accuracy: 0.9055\n",
            "Epoch 14/200\n",
            "1536/1536 [==============================] - 3136s 2s/step - loss: 0.2260 - accuracy: 0.9025 - val_loss: 0.2194 - val_accuracy: 0.9043\n",
            "Epoch 15/200\n",
            "1536/1536 [==============================] - 3135s 2s/step - loss: 0.2250 - accuracy: 0.9030 - val_loss: 0.2219 - val_accuracy: 0.9033\n",
            "Epoch 16/200\n",
            "1536/1536 [==============================] - 3141s 2s/step - loss: 0.2260 - accuracy: 0.9026 - val_loss: 0.2198 - val_accuracy: 0.9039\n",
            "Epoch 17/200\n",
            "1536/1536 [==============================] - 3137s 2s/step - loss: 0.2236 - accuracy: 0.9032 - val_loss: 0.2197 - val_accuracy: 0.9042\n",
            "Epoch 18/200\n",
            "1536/1536 [==============================] - 3143s 2s/step - loss: 0.2304 - accuracy: 0.9009 - val_loss: 0.2177 - val_accuracy: 0.9050\n",
            "Epoch 19/200\n",
            "1536/1536 [==============================] - 3143s 2s/step - loss: 0.2200 - accuracy: 0.9051 - val_loss: 0.2244 - val_accuracy: 0.9033\n",
            "Epoch 20/200\n",
            "1536/1536 [==============================] - 3139s 2s/step - loss: 0.2224 - accuracy: 0.9039 - val_loss: 0.2196 - val_accuracy: 0.9034\n",
            "Epoch 21/200\n",
            "1536/1536 [==============================] - 3162s 2s/step - loss: 0.2222 - accuracy: 0.9040 - val_loss: 0.2213 - val_accuracy: 0.9040\n",
            "Epoch 22/200\n",
            "1536/1536 [==============================] - 3190s 2s/step - loss: 0.2213 - accuracy: 0.9043 - val_loss: 0.2194 - val_accuracy: 0.9042\n",
            "Epoch 23/200\n",
            "1536/1536 [==============================] - 3191s 2s/step - loss: 0.2220 - accuracy: 0.9036 - val_loss: 0.2225 - val_accuracy: 0.9036\n",
            "Epoch 24/200\n",
            "1536/1536 [==============================] - 3184s 2s/step - loss: 0.2215 - accuracy: 0.9042 - val_loss: 0.2212 - val_accuracy: 0.9040\n",
            "Epoch 25/200\n",
            "1536/1536 [==============================] - 3174s 2s/step - loss: 0.2216 - accuracy: 0.9042 - val_loss: 0.2187 - val_accuracy: 0.9055\n",
            "Epoch 26/200\n",
            "1536/1536 [==============================] - 3170s 2s/step - loss: 0.2196 - accuracy: 0.9049 - val_loss: 0.2179 - val_accuracy: 0.9046\n",
            "Epoch 27/200\n",
            "1536/1536 [==============================] - 3174s 2s/step - loss: 0.2204 - accuracy: 0.9048 - val_loss: 0.2166 - val_accuracy: 0.9058\n",
            "Epoch 28/200\n",
            "1536/1536 [==============================] - 3168s 2s/step - loss: 0.2202 - accuracy: 0.9050 - val_loss: 0.2162 - val_accuracy: 0.9059\n",
            "Epoch 29/200\n",
            "1536/1536 [==============================] - 3049s 2s/step - loss: 0.2188 - accuracy: 0.9053 - val_loss: 0.2167 - val_accuracy: 0.9055\n",
            "Epoch 30/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2269 - accuracy: 0.9021 - val_loss: 0.2166 - val_accuracy: 0.9054\n",
            "Epoch 31/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2161 - accuracy: 0.9066 - val_loss: 0.2255 - val_accuracy: 0.9015\n",
            "Epoch 32/200\n",
            "1536/1536 [==============================] - 3027s 2s/step - loss: 0.2173 - accuracy: 0.9059 - val_loss: 0.2201 - val_accuracy: 0.9047\n",
            "Epoch 33/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2175 - accuracy: 0.9056 - val_loss: 0.2217 - val_accuracy: 0.9036\n",
            "Epoch 34/200\n",
            "1536/1536 [==============================] - 3025s 2s/step - loss: 0.2177 - accuracy: 0.9054 - val_loss: 0.2204 - val_accuracy: 0.9046\n",
            "Epoch 35/200\n",
            "1536/1536 [==============================] - 3024s 2s/step - loss: 0.2178 - accuracy: 0.9058 - val_loss: 0.2192 - val_accuracy: 0.9050\n",
            "Epoch 36/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2244 - accuracy: 0.9034 - val_loss: 0.2212 - val_accuracy: 0.9043\n",
            "Epoch 37/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2150 - accuracy: 0.9068 - val_loss: 0.2190 - val_accuracy: 0.9052\n",
            "Epoch 38/200\n",
            "1536/1536 [==============================] - 3025s 2s/step - loss: 0.2172 - accuracy: 0.9059 - val_loss: 0.2179 - val_accuracy: 0.9054\n",
            "Epoch 39/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2161 - accuracy: 0.9062 - val_loss: 0.2228 - val_accuracy: 0.9032\n",
            "Epoch 40/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2158 - accuracy: 0.9066 - val_loss: 0.2158 - val_accuracy: 0.9064\n",
            "Epoch 41/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2167 - accuracy: 0.9067 - val_loss: 0.2249 - val_accuracy: 0.9036\n",
            "Epoch 42/200\n",
            "1536/1536 [==============================] - 3030s 2s/step - loss: 0.2147 - accuracy: 0.9070 - val_loss: 0.2255 - val_accuracy: 0.9023\n",
            "Epoch 43/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2159 - accuracy: 0.9066 - val_loss: 0.2186 - val_accuracy: 0.9059\n",
            "Epoch 44/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2165 - accuracy: 0.9064 - val_loss: 0.2197 - val_accuracy: 0.9048\n",
            "Epoch 45/200\n",
            "1536/1536 [==============================] - 3031s 2s/step - loss: 0.2147 - accuracy: 0.9070 - val_loss: 0.2217 - val_accuracy: 0.9047\n",
            "Epoch 46/200\n",
            "1536/1536 [==============================] - 3030s 2s/step - loss: 0.2153 - accuracy: 0.9066 - val_loss: 0.2217 - val_accuracy: 0.9045\n",
            "Epoch 47/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2154 - accuracy: 0.9068 - val_loss: 0.2178 - val_accuracy: 0.9059\n",
            "Epoch 48/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2144 - accuracy: 0.9071 - val_loss: 0.2206 - val_accuracy: 0.9055\n",
            "Epoch 49/200\n",
            "1536/1536 [==============================] - 3031s 2s/step - loss: 0.2163 - accuracy: 0.9066 - val_loss: 0.2183 - val_accuracy: 0.9059\n",
            "Epoch 50/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2122 - accuracy: 0.9078 - val_loss: 0.2176 - val_accuracy: 0.9064\n",
            "Epoch 51/200\n",
            "1536/1536 [==============================] - 3027s 2s/step - loss: 0.2134 - accuracy: 0.9076 - val_loss: 0.2192 - val_accuracy: 0.9057\n",
            "Epoch 52/200\n",
            "1536/1536 [==============================] - 3027s 2s/step - loss: 0.2135 - accuracy: 0.9076 - val_loss: 0.2274 - val_accuracy: 0.9023\n",
            "Epoch 53/200\n",
            "1536/1536 [==============================] - 3025s 2s/step - loss: 0.2193 - accuracy: 0.9057 - val_loss: 0.2180 - val_accuracy: 0.9058\n",
            "Epoch 54/200\n",
            "1536/1536 [==============================] - 3024s 2s/step - loss: 0.2112 - accuracy: 0.9084 - val_loss: 0.2173 - val_accuracy: 0.9054\n",
            "Epoch 55/200\n",
            "1536/1536 [==============================] - 3025s 2s/step - loss: 0.2132 - accuracy: 0.9075 - val_loss: 0.2273 - val_accuracy: 0.9027\n",
            "Epoch 56/200\n",
            "1536/1536 [==============================] - 3026s 2s/step - loss: 0.2125 - accuracy: 0.9083 - val_loss: 0.2244 - val_accuracy: 0.9033\n",
            "Epoch 57/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2131 - accuracy: 0.9081 - val_loss: 0.2204 - val_accuracy: 0.9056\n",
            "Epoch 58/200\n",
            "1536/1536 [==============================] - 3026s 2s/step - loss: 0.2139 - accuracy: 0.9075 - val_loss: 0.2177 - val_accuracy: 0.9059\n",
            "Epoch 59/200\n",
            "1536/1536 [==============================] - 3024s 2s/step - loss: 0.2115 - accuracy: 0.9085 - val_loss: 0.2314 - val_accuracy: 0.9021\n",
            "Epoch 60/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2133 - accuracy: 0.9074 - val_loss: 0.2206 - val_accuracy: 0.9052\n",
            "Epoch 61/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2110 - accuracy: 0.9086 - val_loss: 0.2199 - val_accuracy: 0.9046\n",
            "Epoch 62/200\n",
            "1536/1536 [==============================] - 3028s 2s/step - loss: 0.2144 - accuracy: 0.9071 - val_loss: 0.2319 - val_accuracy: 0.9014\n",
            "Epoch 63/200\n",
            "1536/1536 [==============================] - 3033s 2s/step - loss: 0.2197 - accuracy: 0.9050 - val_loss: 0.2195 - val_accuracy: 0.9053\n",
            "Epoch 64/200\n",
            "1536/1536 [==============================] - 3032s 2s/step - loss: 0.2082 - accuracy: 0.9096 - val_loss: 0.2225 - val_accuracy: 0.9047\n",
            "Epoch 65/200\n",
            "1536/1536 [==============================] - 3034s 2s/step - loss: 0.2101 - accuracy: 0.9089 - val_loss: 0.2237 - val_accuracy: 0.9043\n",
            "Epoch 66/200\n",
            "1536/1536 [==============================] - 3035s 2s/step - loss: 0.2118 - accuracy: 0.9081 - val_loss: 0.2200 - val_accuracy: 0.9054\n",
            "Epoch 67/200\n",
            "1536/1536 [==============================] - 3031s 2s/step - loss: 0.2201 - accuracy: 0.9051 - val_loss: 0.2159 - val_accuracy: 0.9069\n",
            "Epoch 68/200\n",
            "1536/1536 [==============================] - 3032s 2s/step - loss: 0.2086 - accuracy: 0.9096 - val_loss: 0.2190 - val_accuracy: 0.9050\n",
            "Epoch 69/200\n",
            "1536/1536 [==============================] - 3029s 2s/step - loss: 0.2099 - accuracy: 0.9090 - val_loss: 0.2232 - val_accuracy: 0.9039\n",
            "Epoch 70/200\n",
            "1536/1536 [==============================] - 3031s 2s/step - loss: 0.2103 - accuracy: 0.9087 - val_loss: 0.2295 - val_accuracy: 0.9020\n",
            "Epoch 71/200\n",
            "1536/1536 [==============================] - 3031s 2s/step - loss: 0.2113 - accuracy: 0.9085 - val_loss: 0.2209 - val_accuracy: 0.9063\n",
            "Epoch 72/200\n",
            "1536/1536 [==============================] - 3032s 2s/step - loss: 0.2102 - accuracy: 0.9087 - val_loss: 0.2235 - val_accuracy: 0.9049\n",
            "Epoch 73/200\n",
            "1536/1536 [==============================] - 3030s 2s/step - loss: 0.2219 - accuracy: 0.9043 - val_loss: 0.2220 - val_accuracy: 0.9044\n",
            "Epoch 74/200\n",
            "1536/1536 [==============================] - 3032s 2s/step - loss: 0.2063 - accuracy: 0.9104 - val_loss: 0.2333 - val_accuracy: 0.9010\n",
            "Epoch 75/200\n",
            "1536/1536 [==============================] - 3031s 2s/step - loss: 0.2085 - accuracy: 0.9096 - val_loss: 0.2246 - val_accuracy: 0.9053\n",
            "Epoch 76/200\n",
            "1536/1536 [==============================] - 3030s 2s/step - loss: 0.2101 - accuracy: 0.9091 - val_loss: 0.2189 - val_accuracy: 0.9065\n",
            "Epoch 77/200\n",
            "1536/1536 [==============================] - 3034s 2s/step - loss: 0.2087 - accuracy: 0.9095 - val_loss: 0.2225 - val_accuracy: 0.9042\n",
            "Epoch 78/200\n",
            "1536/1536 [==============================] - 3032s 2s/step - loss: 0.2147 - accuracy: 0.9073 - val_loss: 0.2167 - val_accuracy: 0.9068\n",
            "Epoch 79/200\n",
            "1536/1536 [==============================] - 3026s 2s/step - loss: 0.2066 - accuracy: 0.9103 - val_loss: 0.2193 - val_accuracy: 0.9055\n",
            "Epoch 80/200\n",
            "1536/1536 [==============================] - 3030s 2s/step - loss: 0.2094 - accuracy: 0.9094 - val_loss: 0.2212 - val_accuracy: 0.9045\n",
            "Epoch 81/200\n",
            "1536/1536 [==============================] - 3127s 2s/step - loss: 0.2096 - accuracy: 0.9090 - val_loss: 0.2182 - val_accuracy: 0.9059\n",
            "Epoch 82/200\n",
            "1536/1536 [==============================] - 3164s 2s/step - loss: 0.2697 - accuracy: 0.8910 - val_loss: 0.2208 - val_accuracy: 0.9053\n",
            "Epoch 83/200\n",
            "1536/1536 [==============================] - 3168s 2s/step - loss: 0.2035 - accuracy: 0.9116 - val_loss: 0.2181 - val_accuracy: 0.9059\n",
            "Epoch 84/200\n",
            "1536/1536 [==============================] - 3167s 2s/step - loss: 0.2032 - accuracy: 0.9117 - val_loss: 0.2260 - val_accuracy: 0.9026\n",
            "Epoch 85/200\n",
            "1536/1536 [==============================] - 3164s 2s/step - loss: 0.2088 - accuracy: 0.9098 - val_loss: 0.2225 - val_accuracy: 0.9051\n",
            "Epoch 86/200\n",
            "1536/1536 [==============================] - 3165s 2s/step - loss: 0.2066 - accuracy: 0.9100 - val_loss: 0.2222 - val_accuracy: 0.9051\n",
            "Epoch 87/200\n",
            "1536/1536 [==============================] - 3161s 2s/step - loss: 0.2079 - accuracy: 0.9098 - val_loss: 0.2186 - val_accuracy: 0.9057\n",
            "Epoch 88/200\n",
            "1536/1536 [==============================] - 3160s 2s/step - loss: 0.2087 - accuracy: 0.9091 - val_loss: 0.2249 - val_accuracy: 0.9040\n",
            "Epoch 89/200\n",
            "1536/1536 [==============================] - 3165s 2s/step - loss: 0.2093 - accuracy: 0.9090 - val_loss: 0.2353 - val_accuracy: 0.9020\n",
            "Epoch 90/200\n",
            "1536/1536 [==============================] - 3163s 2s/step - loss: 0.2080 - accuracy: 0.9099 - val_loss: 0.2238 - val_accuracy: 0.9045\n",
            "Epoch 91/200\n",
            "1536/1536 [==============================] - 3162s 2s/step - loss: 0.2084 - accuracy: 0.9098 - val_loss: 0.2207 - val_accuracy: 0.9061\n",
            "Epoch 92/200\n",
            "1536/1536 [==============================] - 3162s 2s/step - loss: 0.2084 - accuracy: 0.9101 - val_loss: 0.2234 - val_accuracy: 0.9055\n",
            "Epoch 93/200\n",
            "1536/1536 [==============================] - 3163s 2s/step - loss: 0.2081 - accuracy: 0.9098 - val_loss: 0.2333 - val_accuracy: 0.8990\n",
            "Epoch 94/200\n",
            "1536/1536 [==============================] - 3163s 2s/step - loss: 0.2079 - accuracy: 0.9097 - val_loss: 0.2222 - val_accuracy: 0.9052\n",
            "Epoch 95/200\n",
            "1536/1536 [==============================] - 3164s 2s/step - loss: 0.2126 - accuracy: 0.9084 - val_loss: 0.2217 - val_accuracy: 0.9063\n",
            "Epoch 96/200\n",
            "1536/1536 [==============================] - 3159s 2s/step - loss: 0.2048 - accuracy: 0.9112 - val_loss: 0.2201 - val_accuracy: 0.9058\n",
            "Epoch 97/200\n",
            "1536/1536 [==============================] - 3161s 2s/step - loss: 0.2078 - accuracy: 0.9098 - val_loss: 0.2257 - val_accuracy: 0.9040\n",
            "Epoch 98/200\n",
            "1536/1536 [==============================] - 3164s 2s/step - loss: 0.2073 - accuracy: 0.9099 - val_loss: 0.2301 - val_accuracy: 0.9023\n",
            "Epoch 99/200\n",
            "1536/1536 [==============================] - 3164s 2s/step - loss: 0.2072 - accuracy: 0.9100 - val_loss: 0.2267 - val_accuracy: 0.9034\n",
            "Epoch 100/200\n",
            "1536/1536 [==============================] - 3166s 2s/step - loss: 0.2075 - accuracy: 0.9101 - val_loss: 0.2295 - val_accuracy: 0.9021\n",
            "Epoch 101/200\n",
            "1536/1536 [==============================] - 3165s 2s/step - loss: 0.2078 - accuracy: 0.9099 - val_loss: 0.2229 - val_accuracy: 0.9040\n",
            "Epoch 102/200\n",
            "1536/1536 [==============================] - 3165s 2s/step - loss: 0.2631 - accuracy: 0.8895 - val_loss: 0.2302 - val_accuracy: 0.9013\n",
            "Epoch 103/200\n",
            "1536/1536 [==============================] - 3161s 2s/step - loss: 0.2057 - accuracy: 0.9105 - val_loss: 0.2263 - val_accuracy: 0.9047\n",
            "Epoch 104/200\n",
            "1536/1536 [==============================] - 3198s 2s/step - loss: 0.2007 - accuracy: 0.9126 - val_loss: 0.2271 - val_accuracy: 0.9050\n",
            "Epoch 105/200\n",
            "1536/1536 [==============================] - 3201s 2s/step - loss: 0.2044 - accuracy: 0.9113 - val_loss: 0.2283 - val_accuracy: 0.9046\n",
            "Epoch 106/200\n",
            "1536/1536 [==============================] - 3200s 2s/step - loss: 0.2070 - accuracy: 0.9101 - val_loss: 0.2242 - val_accuracy: 0.9040\n",
            "Epoch 107/200\n",
            "1536/1536 [==============================] - 3202s 2s/step - loss: 0.2066 - accuracy: 0.9101 - val_loss: 0.2265 - val_accuracy: 0.9039\n",
            "Epoch 108/200\n",
            "1536/1536 [==============================] - 3202s 2s/step - loss: 0.2190 - accuracy: 0.9065 - val_loss: 0.2196 - val_accuracy: 0.9069\n",
            "Epoch 109/200\n",
            "1536/1536 [==============================] - 3201s 2s/step - loss: 0.2122 - accuracy: 0.9081 - val_loss: 0.2231 - val_accuracy: 0.9052\n",
            "Epoch 110/200\n",
            "1536/1536 [==============================] - 3204s 2s/step - loss: 0.2038 - accuracy: 0.9114 - val_loss: 0.2219 - val_accuracy: 0.9047\n",
            "Epoch 111/200\n",
            "1536/1536 [==============================] - 3201s 2s/step - loss: 0.2043 - accuracy: 0.9112 - val_loss: 0.2235 - val_accuracy: 0.9051\n",
            "Epoch 112/200\n",
            "1536/1536 [==============================] - 3205s 2s/step - loss: 0.2058 - accuracy: 0.9110 - val_loss: 0.2305 - val_accuracy: 0.9030\n",
            "Epoch 113/200\n",
            "1536/1536 [==============================] - 3202s 2s/step - loss: 0.2044 - accuracy: 0.9113 - val_loss: 0.2277 - val_accuracy: 0.9044\n",
            "Epoch 114/200\n",
            "1536/1536 [==============================] - 3202s 2s/step - loss: 0.2063 - accuracy: 0.9104 - val_loss: 0.2360 - val_accuracy: 0.8999\n",
            "Epoch 115/200\n",
            "1536/1536 [==============================] - 3206s 2s/step - loss: 0.2049 - accuracy: 0.9110 - val_loss: 0.2231 - val_accuracy: 0.9049\n",
            "Epoch 116/200\n",
            "1536/1536 [==============================] - 3200s 2s/step - loss: 0.2051 - accuracy: 0.9110 - val_loss: 0.2271 - val_accuracy: 0.9043\n",
            "Epoch 117/200\n",
            "1536/1536 [==============================] - 3202s 2s/step - loss: 0.2052 - accuracy: 0.9110 - val_loss: 0.2241 - val_accuracy: 0.9052\n",
            "Epoch 118/200\n",
            "1536/1536 [==============================] - 3204s 2s/step - loss: 0.2051 - accuracy: 0.9110 - val_loss: 0.2244 - val_accuracy: 0.9044\n",
            "Epoch 119/200\n",
            "1536/1536 [==============================] - 3205s 2s/step - loss: 0.2183 - accuracy: 0.9064 - val_loss: 0.2257 - val_accuracy: 0.9046\n",
            "Epoch 120/200\n",
            "1536/1536 [==============================] - 3199s 2s/step - loss: 0.2013 - accuracy: 0.9123 - val_loss: 0.2277 - val_accuracy: 0.9045\n",
            "Epoch 121/200\n",
            "1536/1536 [==============================] - 3199s 2s/step - loss: 0.2060 - accuracy: 0.9107 - val_loss: 0.2233 - val_accuracy: 0.9048\n",
            "Epoch 122/200\n",
            "1536/1536 [==============================] - 3200s 2s/step - loss: 0.2037 - accuracy: 0.9113 - val_loss: 0.2299 - val_accuracy: 0.9030\n",
            "Epoch 123/200\n",
            "1536/1536 [==============================] - 3200s 2s/step - loss: 0.2039 - accuracy: 0.9113 - val_loss: 0.2356 - val_accuracy: 0.8995\n",
            "Epoch 124/200\n",
            "1536/1536 [==============================] - 3205s 2s/step - loss: 0.2038 - accuracy: 0.9114 - val_loss: 0.2249 - val_accuracy: 0.9053\n",
            "Epoch 125/200\n",
            "1536/1536 [==============================] - 3205s 2s/step - loss: 0.2141 - accuracy: 0.9076 - val_loss: 0.2254 - val_accuracy: 0.9041\n",
            "Epoch 126/200\n",
            "1536/1536 [==============================] - 3206s 2s/step - loss: 0.2012 - accuracy: 0.9126 - val_loss: 0.2248 - val_accuracy: 0.9057\n",
            "Epoch 127/200\n",
            "1536/1536 [==============================] - 3201s 2s/step - loss: 0.2032 - accuracy: 0.9118 - val_loss: 0.2253 - val_accuracy: 0.9043\n",
            "Epoch 128/200\n",
            "1536/1536 [==============================] - 3207s 2s/step - loss: 0.2039 - accuracy: 0.9116 - val_loss: 0.3251 - val_accuracy: 0.8821\n",
            "Epoch 129/200\n",
            "1536/1536 [==============================] - 3200s 2s/step - loss: 0.2082 - accuracy: 0.9098 - val_loss: 0.2218 - val_accuracy: 0.9065\n",
            "Epoch 130/200\n",
            "1536/1536 [==============================] - 3204s 2s/step - loss: 0.2036 - accuracy: 0.9116 - val_loss: 0.2249 - val_accuracy: 0.9059\n",
            "Epoch 131/200\n",
            "1536/1536 [==============================] - 3206s 2s/step - loss: 0.2037 - accuracy: 0.9115 - val_loss: 0.2249 - val_accuracy: 0.9042\n",
            "Epoch 132/200\n",
            "1536/1536 [==============================] - 3201s 2s/step - loss: 0.2101 - accuracy: 0.9095 - val_loss: 0.2259 - val_accuracy: 0.9049\n",
            "Epoch 133/200\n",
            "1536/1536 [==============================] - 3202s 2s/step - loss: 0.1991 - accuracy: 0.9132 - val_loss: 0.2256 - val_accuracy: 0.9041\n",
            "Epoch 134/200\n",
            "1536/1536 [==============================] - 3200s 2s/step - loss: 0.2037 - accuracy: 0.9116 - val_loss: 0.2246 - val_accuracy: 0.9049\n",
            "Epoch 135/200\n",
            "1536/1536 [==============================] - 3193s 2s/step - loss: 0.2044 - accuracy: 0.9114 - val_loss: 0.2213 - val_accuracy: 0.9054\n",
            "Epoch 136/200\n",
            "1536/1536 [==============================] - 3183s 2s/step - loss: 0.2028 - accuracy: 0.9116 - val_loss: 0.2338 - val_accuracy: 0.9015\n",
            "Epoch 137/200\n",
            "1536/1536 [==============================] - 3190s 2s/step - loss: 0.2043 - accuracy: 0.9110 - val_loss: 0.2256 - val_accuracy: 0.9046\n",
            "Epoch 138/200\n",
            "1536/1536 [==============================] - 3190s 2s/step - loss: 0.2032 - accuracy: 0.9117 - val_loss: 0.2281 - val_accuracy: 0.9040\n",
            "Epoch 139/200\n",
            "1536/1536 [==============================] - 3192s 2s/step - loss: 0.2038 - accuracy: 0.9112 - val_loss: 0.2273 - val_accuracy: 0.9026\n",
            "Epoch 140/200\n",
            "1536/1536 [==============================] - 3187s 2s/step - loss: 0.2159 - accuracy: 0.9078 - val_loss: 0.2417 - val_accuracy: 0.8967\n",
            "Epoch 141/200\n",
            "1536/1536 [==============================] - 3188s 2s/step - loss: 0.2030 - accuracy: 0.9119 - val_loss: 0.2257 - val_accuracy: 0.9059\n",
            "Epoch 142/200\n",
            "1536/1536 [==============================] - 3187s 2s/step - loss: 0.1991 - accuracy: 0.9135 - val_loss: 0.2270 - val_accuracy: 0.9054\n",
            "Epoch 143/200\n",
            "1536/1536 [==============================] - 3191s 2s/step - loss: 0.2021 - accuracy: 0.9117 - val_loss: 0.2248 - val_accuracy: 0.9048\n",
            "Epoch 144/200\n",
            "1536/1536 [==============================] - 3188s 2s/step - loss: 0.2038 - accuracy: 0.9117 - val_loss: 0.2296 - val_accuracy: 0.9046\n",
            "Epoch 145/200\n",
            "1536/1536 [==============================] - 3193s 2s/step - loss: 0.2023 - accuracy: 0.9121 - val_loss: 0.2236 - val_accuracy: 0.9058\n",
            "Epoch 146/200\n",
            "1536/1536 [==============================] - 3193s 2s/step - loss: 0.2023 - accuracy: 0.9120 - val_loss: 0.2262 - val_accuracy: 0.9044\n",
            "Epoch 147/200\n",
            "1536/1536 [==============================] - 3188s 2s/step - loss: 0.2218 - accuracy: 0.9049 - val_loss: 0.2277 - val_accuracy: 0.9047\n",
            "Epoch 148/200\n",
            "1536/1536 [==============================] - 3189s 2s/step - loss: 0.2003 - accuracy: 0.9130 - val_loss: 0.2266 - val_accuracy: 0.9043\n",
            "Epoch 149/200\n",
            "1536/1536 [==============================] - 3186s 2s/step - loss: 0.1988 - accuracy: 0.9137 - val_loss: 0.2313 - val_accuracy: 0.9031\n",
            "Epoch 150/200\n",
            "1536/1536 [==============================] - 3185s 2s/step - loss: 0.2341 - accuracy: 0.9017 - val_loss: 0.2275 - val_accuracy: 0.9054\n",
            "Epoch 151/200\n",
            "1536/1536 [==============================] - 3194s 2s/step - loss: 0.1939 - accuracy: 0.9154 - val_loss: 0.2286 - val_accuracy: 0.9048\n",
            "Epoch 152/200\n",
            "1536/1536 [==============================] - 3190s 2s/step - loss: 0.1963 - accuracy: 0.9147 - val_loss: 0.2271 - val_accuracy: 0.9054\n",
            "Epoch 153/200\n",
            "1536/1536 [==============================] - 3190s 2s/step - loss: 0.1998 - accuracy: 0.9131 - val_loss: 0.2258 - val_accuracy: 0.9049\n",
            "Epoch 154/200\n",
            "1536/1536 [==============================] - 3188s 2s/step - loss: 0.2025 - accuracy: 0.9124 - val_loss: 0.2242 - val_accuracy: 0.9044\n",
            "Epoch 155/200\n",
            "1536/1536 [==============================] - 3195s 2s/step - loss: 0.2015 - accuracy: 0.9125 - val_loss: 0.2239 - val_accuracy: 0.9052\n",
            "Epoch 156/200\n",
            "1536/1536 [==============================] - 3185s 2s/step - loss: 0.2012 - accuracy: 0.9127 - val_loss: 0.2285 - val_accuracy: 0.9047\n",
            "Epoch 157/200\n",
            "1536/1536 [==============================] - 3187s 2s/step - loss: 0.2017 - accuracy: 0.9125 - val_loss: 0.2321 - val_accuracy: 0.9020\n",
            "Epoch 158/200\n",
            "1536/1536 [==============================] - 3192s 2s/step - loss: 0.2019 - accuracy: 0.9123 - val_loss: 0.2335 - val_accuracy: 0.9017\n",
            "Epoch 159/200\n",
            "1536/1536 [==============================] - 3192s 2s/step - loss: 0.2052 - accuracy: 0.9111 - val_loss: 0.2251 - val_accuracy: 0.9059\n",
            "Epoch 160/200\n",
            "1536/1536 [==============================] - 3193s 2s/step - loss: 0.2001 - accuracy: 0.9130 - val_loss: 0.2297 - val_accuracy: 0.9041\n",
            "Epoch 161/200\n",
            "1536/1536 [==============================] - 3188s 2s/step - loss: 0.2011 - accuracy: 0.9127 - val_loss: 0.2283 - val_accuracy: 0.9042\n",
            "Epoch 162/200\n",
            "1536/1536 [==============================] - 3314s 2s/step - loss: 0.2006 - accuracy: 0.9127 - val_loss: 0.2369 - val_accuracy: 0.9014\n",
            "Epoch 163/200\n",
            "1536/1536 [==============================] - 3367s 2s/step - loss: 0.2023 - accuracy: 0.9124 - val_loss: 0.2268 - val_accuracy: 0.9047\n",
            "Epoch 164/200\n",
            "1536/1536 [==============================] - 3377s 2s/step - loss: 0.2037 - accuracy: 0.9118 - val_loss: 0.2283 - val_accuracy: 0.9033\n",
            "Epoch 165/200\n",
            "1536/1536 [==============================] - 3366s 2s/step - loss: 0.1994 - accuracy: 0.9132 - val_loss: 0.2276 - val_accuracy: 0.9039\n",
            "Epoch 166/200\n",
            "1536/1536 [==============================] - 3373s 2s/step - loss: 0.1992 - accuracy: 0.9134 - val_loss: 0.2247 - val_accuracy: 0.9049\n",
            "Epoch 167/200\n",
            "1536/1536 [==============================] - 3219s 2s/step - loss: 0.2057 - accuracy: 0.9112 - val_loss: 0.2330 - val_accuracy: 0.9041\n",
            "Epoch 168/200\n",
            "1536/1536 [==============================] - 3295s 2s/step - loss: 0.1980 - accuracy: 0.9137 - val_loss: 0.2295 - val_accuracy: 0.9047\n",
            "Epoch 169/200\n",
            "1536/1536 [==============================] - 3295s 2s/step - loss: 0.2011 - accuracy: 0.9128 - val_loss: 0.2275 - val_accuracy: 0.9048\n",
            "Epoch 170/200\n",
            "1536/1536 [==============================] - 3295s 2s/step - loss: 0.1992 - accuracy: 0.9133 - val_loss: 0.2300 - val_accuracy: 0.9035\n",
            "Epoch 171/200\n",
            "1536/1536 [==============================] - 3292s 2s/step - loss: 0.2027 - accuracy: 0.9120 - val_loss: 0.2309 - val_accuracy: 0.9038\n",
            "Epoch 172/200\n",
            "1536/1536 [==============================] - 3295s 2s/step - loss: 0.2011 - accuracy: 0.9126 - val_loss: 0.2308 - val_accuracy: 0.9049\n",
            "Epoch 173/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.2010 - accuracy: 0.9128 - val_loss: 0.2299 - val_accuracy: 0.9024\n",
            "Epoch 174/200\n",
            "1536/1536 [==============================] - 3296s 2s/step - loss: 0.2011 - accuracy: 0.9128 - val_loss: 0.2307 - val_accuracy: 0.9042\n",
            "Epoch 175/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.2007 - accuracy: 0.9128 - val_loss: 0.2306 - val_accuracy: 0.9041\n",
            "Epoch 176/200\n",
            "1536/1536 [==============================] - 3292s 2s/step - loss: 0.2030 - accuracy: 0.9122 - val_loss: 0.2341 - val_accuracy: 0.9016\n",
            "Epoch 177/200\n",
            "1536/1536 [==============================] - 3295s 2s/step - loss: 0.2005 - accuracy: 0.9132 - val_loss: 0.2276 - val_accuracy: 0.9054\n",
            "Epoch 178/200\n",
            "1536/1536 [==============================] - 3303s 2s/step - loss: 0.1995 - accuracy: 0.9132 - val_loss: 0.2377 - val_accuracy: 0.9038\n",
            "Epoch 179/200\n",
            "1536/1536 [==============================] - 3297s 2s/step - loss: 0.2001 - accuracy: 0.9129 - val_loss: 0.2352 - val_accuracy: 0.9034\n",
            "Epoch 180/200\n",
            "1536/1536 [==============================] - 3296s 2s/step - loss: 0.1999 - accuracy: 0.9130 - val_loss: 0.2351 - val_accuracy: 0.9044\n",
            "Epoch 181/200\n",
            "1536/1536 [==============================] - 3301s 2s/step - loss: 0.2051 - accuracy: 0.9110 - val_loss: 0.2278 - val_accuracy: 0.9052\n",
            "Epoch 182/200\n",
            "1536/1536 [==============================] - 3301s 2s/step - loss: 0.2024 - accuracy: 0.9124 - val_loss: 0.2276 - val_accuracy: 0.9054\n",
            "Epoch 183/200\n",
            "1536/1536 [==============================] - 3295s 2s/step - loss: 0.1946 - accuracy: 0.9151 - val_loss: 0.2294 - val_accuracy: 0.9046\n",
            "Epoch 184/200\n",
            "1536/1536 [==============================] - 3297s 2s/step - loss: 0.1988 - accuracy: 0.9136 - val_loss: 0.2340 - val_accuracy: 0.9019\n",
            "Epoch 185/200\n",
            "1536/1536 [==============================] - 3296s 2s/step - loss: 0.2008 - accuracy: 0.9130 - val_loss: 0.2314 - val_accuracy: 0.9023\n",
            "Epoch 186/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.1994 - accuracy: 0.9131 - val_loss: 0.2283 - val_accuracy: 0.9051\n",
            "Epoch 187/200\n",
            "1536/1536 [==============================] - 3293s 2s/step - loss: 0.2155 - accuracy: 0.9075 - val_loss: 0.2285 - val_accuracy: 0.9038\n",
            "Epoch 188/200\n",
            "1536/1536 [==============================] - 3294s 2s/step - loss: 0.1955 - accuracy: 0.9148 - val_loss: 0.2375 - val_accuracy: 0.9022\n",
            "Epoch 189/200\n",
            "1536/1536 [==============================] - 3296s 2s/step - loss: 0.1970 - accuracy: 0.9144 - val_loss: 0.2363 - val_accuracy: 0.9022\n",
            "Epoch 190/200\n",
            "1536/1536 [==============================] - 3293s 2s/step - loss: 0.1997 - accuracy: 0.9134 - val_loss: 0.2298 - val_accuracy: 0.9046\n",
            "Epoch 191/200\n",
            "1536/1536 [==============================] - 3296s 2s/step - loss: 0.1988 - accuracy: 0.9136 - val_loss: 0.2324 - val_accuracy: 0.9041\n",
            "Epoch 192/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.2005 - accuracy: 0.9130 - val_loss: 0.2701 - val_accuracy: 0.8846\n",
            "Epoch 193/200\n",
            "1536/1536 [==============================] - 3293s 2s/step - loss: 0.1992 - accuracy: 0.9132 - val_loss: 0.2322 - val_accuracy: 0.9033\n",
            "Epoch 194/200\n",
            "1536/1536 [==============================] - 3300s 2s/step - loss: 0.2125 - accuracy: 0.9084 - val_loss: 0.2296 - val_accuracy: 0.9040\n",
            "Epoch 195/200\n",
            "1536/1536 [==============================] - 3298s 2s/step - loss: 0.2056 - accuracy: 0.9115 - val_loss: 0.3241 - val_accuracy: 0.8629\n",
            "Epoch 196/200\n",
            "1536/1536 [==============================] - 3302s 2s/step - loss: 0.2055 - accuracy: 0.9110 - val_loss: 0.2292 - val_accuracy: 0.9055\n",
            "Epoch 197/200\n",
            "1536/1536 [==============================] - 3293s 2s/step - loss: 0.1941 - accuracy: 0.9157 - val_loss: 0.2280 - val_accuracy: 0.9037\n",
            "Epoch 198/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.1984 - accuracy: 0.9137 - val_loss: 0.2302 - val_accuracy: 0.9045\n",
            "Epoch 199/200\n",
            "1536/1536 [==============================] - 3298s 2s/step - loss: 0.1978 - accuracy: 0.9142 - val_loss: 0.2327 - val_accuracy: 0.9035\n",
            "Epoch 200/200\n",
            "1536/1536 [==============================] - 3297s 2s/step - loss: 0.2003 - accuracy: 0.9130 - val_loss: 0.2344 - val_accuracy: 0.9035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/fil-server/.local/lib/python3.8/site-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LModCNNResNet Relu\n",
        "\n",
        "Generate LModCNN with residual connexion architecture as defined in Courtat and du Mas des Bourboux, A light neural network for modulation detection under impairments, ISNCC 2021"
      ],
      "metadata": {
        "id": "BDQB8gTXFSf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_network = \"LModCNNResNetRelu\"\n",
        "\n",
        "dynamic_input_shp = input_shp.copy()\n",
        "dynamic_input_shp[0] = None\n",
        "\n",
        "model = getattr(neural_nets_keras,'get_{}'.format(name_network))(dynamic_input_shp,output_shp)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HTPuClMjFJY_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247fccc2-042d-48fd-f5c1-9b748f24024e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None, 2)]    0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, None, 8)      24          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, None, 8)      456         ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, None, 8)      456         ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " add (Add)                      (None, None, 8)      0           ['conv1d_2[0][0]',               \n",
            "                                                                  'conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, None, 8)      0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, None, 16)     144         ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, None, 16)     1808        ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, None, 16)     1808        ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, None, 16)     0           ['conv1d_5[0][0]',               \n",
            "                                                                  'conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, None, 16)     0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, None, 32)     544         ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, None, 32)     7200        ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, None, 32)     7200        ['conv1d_7[0][0]']               \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, None, 32)     0           ['conv1d_8[0][0]',               \n",
            "                                                                  'conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, None, 32)     0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, None, 64)     2112        ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, None, 64)     28736       ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, None, 64)     28736       ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, None, 64)     0           ['conv1d_11[0][0]',              \n",
            "                                                                  'conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, None, 64)     0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 64)          0           ['activation_3[0][0]']           \n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 256)          16640       ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 24)           6168        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 102,032\n",
            "Trainable params: 102,032\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "train_model(model, name_network)"
      ],
      "metadata": {
        "id": "sDfi6RrkFcGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fd66e9e-61a5-4479-9217-b53e24b410df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1536/1536 [==============================] - 2989s 2s/step - loss: 1.2810 - accuracy: 0.4785 - val_loss: 1.0367 - val_accuracy: 0.5513\n",
            "Epoch 2/200\n",
            "1536/1536 [==============================] - 2988s 2s/step - loss: 0.9188 - accuracy: 0.6089 - val_loss: 0.7052 - val_accuracy: 0.7055\n",
            "Epoch 3/200\n",
            "1536/1536 [==============================] - 2984s 2s/step - loss: 0.7414 - accuracy: 0.7001 - val_loss: 1.4658 - val_accuracy: 0.4566\n",
            "Epoch 4/200\n",
            "1536/1536 [==============================] - 2982s 2s/step - loss: 0.8323 - accuracy: 0.6462 - val_loss: 0.5576 - val_accuracy: 0.7543\n",
            "Epoch 5/200\n",
            "1536/1536 [==============================] - 2984s 2s/step - loss: 0.5875 - accuracy: 0.7451 - val_loss: 0.5107 - val_accuracy: 0.7810\n",
            "Epoch 6/200\n",
            "1536/1536 [==============================] - 2984s 2s/step - loss: 0.5315 - accuracy: 0.7735 - val_loss: 0.4714 - val_accuracy: 0.8020\n",
            "Epoch 7/200\n",
            "1536/1536 [==============================] - 2985s 2s/step - loss: 0.4871 - accuracy: 0.7967 - val_loss: 0.4315 - val_accuracy: 0.8192\n",
            "Epoch 8/200\n",
            "1536/1536 [==============================] - 2986s 2s/step - loss: 0.4513 - accuracy: 0.8116 - val_loss: 0.3884 - val_accuracy: 0.8346\n",
            "Epoch 9/200\n",
            "1536/1536 [==============================] - 2987s 2s/step - loss: 0.5382 - accuracy: 0.7949 - val_loss: 1.0411 - val_accuracy: 0.5753\n",
            "Epoch 10/200\n",
            "1536/1536 [==============================] - 2990s 2s/step - loss: 0.5943 - accuracy: 0.7552 - val_loss: 0.4116 - val_accuracy: 0.8284\n",
            "Epoch 11/200\n",
            "1536/1536 [==============================] - 2988s 2s/step - loss: 0.4412 - accuracy: 0.8160 - val_loss: 0.3837 - val_accuracy: 0.8371\n",
            "Epoch 12/200\n",
            "1536/1536 [==============================] - 2990s 2s/step - loss: 0.4137 - accuracy: 0.8270 - val_loss: 0.3727 - val_accuracy: 0.8391\n",
            "Epoch 13/200\n",
            "1536/1536 [==============================] - 2987s 2s/step - loss: 0.4324 - accuracy: 0.8192 - val_loss: 0.3985 - val_accuracy: 0.8339\n",
            "Epoch 14/200\n",
            "1536/1536 [==============================] - 2992s 2s/step - loss: 0.3968 - accuracy: 0.8349 - val_loss: 0.3550 - val_accuracy: 0.8511\n",
            "Epoch 15/200\n",
            "1536/1536 [==============================] - 2990s 2s/step - loss: 0.3750 - accuracy: 0.8436 - val_loss: 0.3250 - val_accuracy: 0.8634\n",
            "Epoch 16/200\n",
            "1536/1536 [==============================] - 2987s 2s/step - loss: 0.3583 - accuracy: 0.8501 - val_loss: 0.3255 - val_accuracy: 0.8637\n",
            "Epoch 17/200\n",
            "1536/1536 [==============================] - 2988s 2s/step - loss: 0.3927 - accuracy: 0.8390 - val_loss: 0.3251 - val_accuracy: 0.8634\n",
            "Epoch 18/200\n",
            "1536/1536 [==============================] - 2992s 2s/step - loss: 0.3356 - accuracy: 0.8599 - val_loss: 0.3004 - val_accuracy: 0.8739\n",
            "Epoch 19/200\n",
            "1536/1536 [==============================] - 2992s 2s/step - loss: 0.3280 - accuracy: 0.8624 - val_loss: 0.3046 - val_accuracy: 0.8713\n",
            "Epoch 20/200\n",
            "1536/1536 [==============================] - 2988s 2s/step - loss: 0.3195 - accuracy: 0.8660 - val_loss: 0.2991 - val_accuracy: 0.8711\n",
            "Epoch 21/200\n",
            "1536/1536 [==============================] - 2986s 2s/step - loss: 0.3116 - accuracy: 0.8694 - val_loss: 0.3062 - val_accuracy: 0.8678\n",
            "Epoch 22/200\n",
            "1536/1536 [==============================] - 2987s 2s/step - loss: 0.3068 - accuracy: 0.8712 - val_loss: 0.2896 - val_accuracy: 0.8772\n",
            "Epoch 23/200\n",
            "1536/1536 [==============================] - 2988s 2s/step - loss: 0.3004 - accuracy: 0.8740 - val_loss: 0.2866 - val_accuracy: 0.8784\n",
            "Epoch 24/200\n",
            "1536/1536 [==============================] - 2986s 2s/step - loss: 0.2967 - accuracy: 0.8752 - val_loss: 0.2859 - val_accuracy: 0.8785\n",
            "Epoch 25/200\n",
            "1536/1536 [==============================] - 2989s 2s/step - loss: 0.3010 - accuracy: 0.8735 - val_loss: 0.2630 - val_accuracy: 0.8870\n",
            "Epoch 26/200\n",
            "1536/1536 [==============================] - 3034s 2s/step - loss: 0.2862 - accuracy: 0.8793 - val_loss: 0.2597 - val_accuracy: 0.8885\n",
            "Epoch 27/200\n",
            "1536/1536 [==============================] - 3128s 2s/step - loss: 0.6277 - accuracy: 0.7493 - val_loss: 0.3021 - val_accuracy: 0.8718\n",
            "Epoch 28/200\n",
            "1536/1536 [==============================] - 3126s 2s/step - loss: 0.3043 - accuracy: 0.8713 - val_loss: 0.2660 - val_accuracy: 0.8857\n",
            "Epoch 29/200\n",
            "1536/1536 [==============================] - 3125s 2s/step - loss: 0.2850 - accuracy: 0.8793 - val_loss: 0.2666 - val_accuracy: 0.8852\n",
            "Epoch 30/200\n",
            "1536/1536 [==============================] - 3126s 2s/step - loss: 0.2995 - accuracy: 0.8745 - val_loss: 0.2676 - val_accuracy: 0.8850\n",
            "Epoch 31/200\n",
            "1536/1536 [==============================] - 3122s 2s/step - loss: 0.2780 - accuracy: 0.8826 - val_loss: 0.2537 - val_accuracy: 0.8913\n",
            "Epoch 32/200\n",
            "1536/1536 [==============================] - 3124s 2s/step - loss: 0.2771 - accuracy: 0.8828 - val_loss: 0.2651 - val_accuracy: 0.8856\n",
            "Epoch 33/200\n",
            "1536/1536 [==============================] - 3120s 2s/step - loss: 0.2852 - accuracy: 0.8798 - val_loss: 0.2561 - val_accuracy: 0.8901\n",
            "Epoch 34/200\n",
            "1536/1536 [==============================] - 3128s 2s/step - loss: 0.2714 - accuracy: 0.8853 - val_loss: 0.2496 - val_accuracy: 0.8927\n",
            "Epoch 35/200\n",
            "1536/1536 [==============================] - 3124s 2s/step - loss: 0.4452 - accuracy: 0.8257 - val_loss: 0.3665 - val_accuracy: 0.8445\n",
            "Epoch 36/200\n",
            "1536/1536 [==============================] - 3123s 2s/step - loss: 0.3290 - accuracy: 0.8622 - val_loss: 0.2793 - val_accuracy: 0.8807\n",
            "Epoch 37/200\n",
            "1536/1536 [==============================] - 3126s 2s/step - loss: 0.2931 - accuracy: 0.8757 - val_loss: 0.2757 - val_accuracy: 0.8811\n",
            "Epoch 38/200\n",
            "1536/1536 [==============================] - 3123s 2s/step - loss: 0.2850 - accuracy: 0.8789 - val_loss: 0.2658 - val_accuracy: 0.8859\n",
            "Epoch 39/200\n",
            "1536/1536 [==============================] - 3119s 2s/step - loss: 0.2836 - accuracy: 0.8803 - val_loss: 0.2642 - val_accuracy: 0.8879\n",
            "Epoch 40/200\n",
            "1536/1536 [==============================] - 3125s 2s/step - loss: 0.3395 - accuracy: 0.8585 - val_loss: 0.2601 - val_accuracy: 0.8877\n",
            "Epoch 41/200\n",
            "1536/1536 [==============================] - 3123s 2s/step - loss: 0.2711 - accuracy: 0.8846 - val_loss: 0.2650 - val_accuracy: 0.8866\n",
            "Epoch 42/200\n",
            "1536/1536 [==============================] - 3126s 2s/step - loss: 0.2724 - accuracy: 0.8842 - val_loss: 0.2478 - val_accuracy: 0.8939\n",
            "Epoch 43/200\n",
            "1536/1536 [==============================] - 3124s 2s/step - loss: 0.2680 - accuracy: 0.8863 - val_loss: 0.2557 - val_accuracy: 0.8910\n",
            "Epoch 44/200\n",
            "1536/1536 [==============================] - 3076s 2s/step - loss: 0.2734 - accuracy: 0.8845 - val_loss: 0.2536 - val_accuracy: 0.8914\n",
            "Epoch 45/200\n",
            "1536/1536 [==============================] - 3080s 2s/step - loss: 0.2646 - accuracy: 0.8877 - val_loss: 0.2511 - val_accuracy: 0.8921\n",
            "Epoch 46/200\n",
            "1536/1536 [==============================] - 3078s 2s/step - loss: 0.2809 - accuracy: 0.8817 - val_loss: 0.2525 - val_accuracy: 0.8908\n",
            "Epoch 47/200\n",
            "1536/1536 [==============================] - 3080s 2s/step - loss: 0.2577 - accuracy: 0.8906 - val_loss: 0.2458 - val_accuracy: 0.8944\n",
            "Epoch 48/200\n",
            "1536/1536 [==============================] - 3077s 2s/step - loss: 0.2620 - accuracy: 0.8890 - val_loss: 0.2456 - val_accuracy: 0.8944\n",
            "Epoch 49/200\n",
            "1536/1536 [==============================] - 3076s 2s/step - loss: 0.2576 - accuracy: 0.8905 - val_loss: 0.2436 - val_accuracy: 0.8953\n",
            "Epoch 50/200\n",
            "1536/1536 [==============================] - 3074s 2s/step - loss: 0.2592 - accuracy: 0.8898 - val_loss: 0.2396 - val_accuracy: 0.8971\n",
            "Epoch 51/200\n",
            "1536/1536 [==============================] - 3075s 2s/step - loss: 0.2557 - accuracy: 0.8914 - val_loss: 0.2439 - val_accuracy: 0.8949\n",
            "Epoch 52/200\n",
            "1536/1536 [==============================] - 3075s 2s/step - loss: 0.2570 - accuracy: 0.8908 - val_loss: 0.2432 - val_accuracy: 0.8952\n",
            "Epoch 53/200\n",
            "1536/1536 [==============================] - 3082s 2s/step - loss: 0.2550 - accuracy: 0.8915 - val_loss: 0.2429 - val_accuracy: 0.8951\n",
            "Epoch 54/200\n",
            "1536/1536 [==============================] - 3086s 2s/step - loss: 0.2522 - accuracy: 0.8927 - val_loss: 0.2465 - val_accuracy: 0.8942\n",
            "Epoch 55/200\n",
            "1536/1536 [==============================] - 3081s 2s/step - loss: 0.3016 - accuracy: 0.8762 - val_loss: 0.2657 - val_accuracy: 0.8861\n",
            "Epoch 56/200\n",
            "1536/1536 [==============================] - 3083s 2s/step - loss: 0.2497 - accuracy: 0.8933 - val_loss: 0.2384 - val_accuracy: 0.8981\n",
            "Epoch 57/200\n",
            "1536/1536 [==============================] - 3080s 2s/step - loss: 0.2457 - accuracy: 0.8951 - val_loss: 0.2378 - val_accuracy: 0.8977\n",
            "Epoch 58/200\n",
            "1536/1536 [==============================] - 3079s 2s/step - loss: 0.2497 - accuracy: 0.8940 - val_loss: 0.2515 - val_accuracy: 0.8911\n",
            "Epoch 59/200\n",
            "1536/1536 [==============================] - 3077s 2s/step - loss: 0.2502 - accuracy: 0.8935 - val_loss: 0.2348 - val_accuracy: 0.8990\n",
            "Epoch 60/200\n",
            "1536/1536 [==============================] - 3079s 2s/step - loss: 0.2473 - accuracy: 0.8944 - val_loss: 0.2460 - val_accuracy: 0.8939\n",
            "Epoch 61/200\n",
            "1536/1536 [==============================] - 3080s 2s/step - loss: 0.2531 - accuracy: 0.8925 - val_loss: 0.3086 - val_accuracy: 0.8701\n",
            "Epoch 62/200\n",
            "1536/1536 [==============================] - 3075s 2s/step - loss: 0.2478 - accuracy: 0.8944 - val_loss: 0.2404 - val_accuracy: 0.8972\n",
            "Epoch 63/200\n",
            "1536/1536 [==============================] - 3077s 2s/step - loss: 0.2462 - accuracy: 0.8949 - val_loss: 0.2365 - val_accuracy: 0.8987\n",
            "Epoch 64/200\n",
            "1536/1536 [==============================] - 3079s 2s/step - loss: 0.2492 - accuracy: 0.8942 - val_loss: 0.2387 - val_accuracy: 0.8979\n",
            "Epoch 65/200\n",
            "1536/1536 [==============================] - 3077s 2s/step - loss: 0.2440 - accuracy: 0.8958 - val_loss: 0.2437 - val_accuracy: 0.8948\n",
            "Epoch 66/200\n",
            "1536/1536 [==============================] - 3076s 2s/step - loss: 0.2451 - accuracy: 0.8951 - val_loss: 0.2334 - val_accuracy: 0.8989\n",
            "Epoch 67/200\n",
            "1536/1536 [==============================] - 3082s 2s/step - loss: 0.2439 - accuracy: 0.8955 - val_loss: 0.2394 - val_accuracy: 0.8969\n",
            "Epoch 68/200\n",
            "1536/1536 [==============================] - 3081s 2s/step - loss: 0.2455 - accuracy: 0.8954 - val_loss: 0.2598 - val_accuracy: 0.8884\n",
            "Epoch 69/200\n",
            "1536/1536 [==============================] - 3230s 2s/step - loss: 0.2420 - accuracy: 0.8969 - val_loss: 0.2308 - val_accuracy: 0.8997\n",
            "Epoch 70/200\n",
            "1536/1536 [==============================] - 3270s 2s/step - loss: 0.2426 - accuracy: 0.8963 - val_loss: 0.2335 - val_accuracy: 0.8995\n",
            "Epoch 71/200\n",
            "1536/1536 [==============================] - 3259s 2s/step - loss: 0.2454 - accuracy: 0.8958 - val_loss: 0.2361 - val_accuracy: 0.8986\n",
            "Epoch 72/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2421 - accuracy: 0.8964 - val_loss: 0.2384 - val_accuracy: 0.8975\n",
            "Epoch 73/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2412 - accuracy: 0.8968 - val_loss: 0.2373 - val_accuracy: 0.8980\n",
            "Epoch 74/200\n",
            "1536/1536 [==============================] - 3261s 2s/step - loss: 0.2425 - accuracy: 0.8968 - val_loss: 0.2460 - val_accuracy: 0.8942\n",
            "Epoch 75/200\n",
            "1536/1536 [==============================] - 3260s 2s/step - loss: 0.2395 - accuracy: 0.8975 - val_loss: 0.2483 - val_accuracy: 0.8948\n",
            "Epoch 76/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2551 - accuracy: 0.8923 - val_loss: 0.2382 - val_accuracy: 0.8980\n",
            "Epoch 77/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2352 - accuracy: 0.8991 - val_loss: 0.2363 - val_accuracy: 0.8988\n",
            "Epoch 78/200\n",
            "1536/1536 [==============================] - 3267s 2s/step - loss: 0.2369 - accuracy: 0.8985 - val_loss: 0.2325 - val_accuracy: 0.8996\n",
            "Epoch 79/200\n",
            "1536/1536 [==============================] - 3263s 2s/step - loss: 0.3243 - accuracy: 0.8686 - val_loss: 0.4645 - val_accuracy: 0.8088\n",
            "Epoch 80/200\n",
            "1536/1536 [==============================] - 3259s 2s/step - loss: 0.2817 - accuracy: 0.8809 - val_loss: 0.2379 - val_accuracy: 0.8974\n",
            "Epoch 81/200\n",
            "1536/1536 [==============================] - 3258s 2s/step - loss: 0.2351 - accuracy: 0.8994 - val_loss: 0.2295 - val_accuracy: 0.9013\n",
            "Epoch 82/200\n",
            "1536/1536 [==============================] - 3262s 2s/step - loss: 0.2345 - accuracy: 0.8996 - val_loss: 0.2367 - val_accuracy: 0.8980\n",
            "Epoch 83/200\n",
            "1536/1536 [==============================] - 3261s 2s/step - loss: 0.2376 - accuracy: 0.8983 - val_loss: 0.2382 - val_accuracy: 0.8981\n",
            "Epoch 84/200\n",
            "1536/1536 [==============================] - 3265s 2s/step - loss: 0.2392 - accuracy: 0.8980 - val_loss: 0.2347 - val_accuracy: 0.8994\n",
            "Epoch 85/200\n",
            "1536/1536 [==============================] - 3267s 2s/step - loss: 0.2357 - accuracy: 0.8990 - val_loss: 0.2363 - val_accuracy: 0.8991\n",
            "Epoch 86/200\n",
            "1536/1536 [==============================] - 3262s 2s/step - loss: 0.2386 - accuracy: 0.8978 - val_loss: 0.2332 - val_accuracy: 0.8990\n",
            "Epoch 87/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2424 - accuracy: 0.8965 - val_loss: 0.2272 - val_accuracy: 0.9021\n",
            "Epoch 88/200\n",
            "1536/1536 [==============================] - 3260s 2s/step - loss: 0.2347 - accuracy: 0.8994 - val_loss: 0.2418 - val_accuracy: 0.8974\n",
            "Epoch 89/200\n",
            "1536/1536 [==============================] - 3258s 2s/step - loss: 0.2360 - accuracy: 0.8986 - val_loss: 0.2320 - val_accuracy: 0.8998\n",
            "Epoch 90/200\n",
            "1536/1536 [==============================] - 3259s 2s/step - loss: 0.2353 - accuracy: 0.8992 - val_loss: 0.2353 - val_accuracy: 0.8994\n",
            "Epoch 91/200\n",
            "1536/1536 [==============================] - 3258s 2s/step - loss: 0.2374 - accuracy: 0.8985 - val_loss: 0.2325 - val_accuracy: 0.8998\n",
            "Epoch 92/200\n",
            "1536/1536 [==============================] - 3260s 2s/step - loss: 0.3221 - accuracy: 0.8747 - val_loss: 0.3928 - val_accuracy: 0.8365\n",
            "Epoch 93/200\n",
            "1536/1536 [==============================] - 3260s 2s/step - loss: 0.2746 - accuracy: 0.8832 - val_loss: 0.2328 - val_accuracy: 0.8999\n",
            "Epoch 94/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2293 - accuracy: 0.9015 - val_loss: 0.2301 - val_accuracy: 0.9013\n",
            "Epoch 95/200\n",
            "1536/1536 [==============================] - 3264s 2s/step - loss: 0.2291 - accuracy: 0.9016 - val_loss: 0.2332 - val_accuracy: 0.8998\n",
            "Epoch 96/200\n",
            "1536/1536 [==============================] - 3261s 2s/step - loss: 0.2328 - accuracy: 0.9000 - val_loss: 0.2448 - val_accuracy: 0.8949\n",
            "Epoch 97/200\n",
            "1536/1536 [==============================] - 3260s 2s/step - loss: 0.2328 - accuracy: 0.9002 - val_loss: 0.2361 - val_accuracy: 0.8987\n",
            "Epoch 98/200\n",
            "1536/1536 [==============================] - 3281s 2s/step - loss: 0.2348 - accuracy: 0.8993 - val_loss: 0.2396 - val_accuracy: 0.8984\n",
            "Epoch 99/200\n",
            "1536/1536 [==============================] - 3298s 2s/step - loss: 0.2358 - accuracy: 0.8991 - val_loss: 0.2311 - val_accuracy: 0.9009\n",
            "Epoch 100/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.2313 - accuracy: 0.9004 - val_loss: 0.2483 - val_accuracy: 0.8940\n",
            "Epoch 101/200\n",
            "1536/1536 [==============================] - 3297s 2s/step - loss: 0.2340 - accuracy: 0.9000 - val_loss: 0.2365 - val_accuracy: 0.8979\n",
            "Epoch 102/200\n",
            "1536/1536 [==============================] - 3291s 2s/step - loss: 0.2330 - accuracy: 0.9000 - val_loss: 0.2358 - val_accuracy: 0.8992\n",
            "Epoch 103/200\n",
            "1536/1536 [==============================] - 3294s 2s/step - loss: 0.2319 - accuracy: 0.9005 - val_loss: 0.2341 - val_accuracy: 0.8992\n",
            "Epoch 104/200\n",
            "1536/1536 [==============================] - 3287s 2s/step - loss: 0.2341 - accuracy: 0.8997 - val_loss: 0.2353 - val_accuracy: 0.8992\n",
            "Epoch 105/200\n",
            "1536/1536 [==============================] - 3287s 2s/step - loss: 0.2314 - accuracy: 0.9007 - val_loss: 0.2295 - val_accuracy: 0.9013\n",
            "Epoch 106/200\n",
            "1536/1536 [==============================] - 3289s 2s/step - loss: 0.2320 - accuracy: 0.9006 - val_loss: 0.2325 - val_accuracy: 0.9001\n",
            "Epoch 107/200\n",
            "1536/1536 [==============================] - 3265s 2s/step - loss: 0.2309 - accuracy: 0.9008 - val_loss: 0.2329 - val_accuracy: 0.9005\n",
            "Epoch 108/200\n",
            "1536/1536 [==============================] - 3260s 2s/step - loss: 0.2339 - accuracy: 0.8999 - val_loss: 0.2295 - val_accuracy: 0.9017\n",
            "Epoch 109/200\n",
            "1536/1536 [==============================] - 3262s 2s/step - loss: 0.2307 - accuracy: 0.9010 - val_loss: 0.2325 - val_accuracy: 0.8993\n",
            "Epoch 110/200\n",
            "1536/1536 [==============================] - 3303s 2s/step - loss: 0.2295 - accuracy: 0.9012 - val_loss: 0.2421 - val_accuracy: 0.8958\n",
            "Epoch 111/200\n",
            "1536/1536 [==============================] - 3323s 2s/step - loss: 0.2307 - accuracy: 0.9008 - val_loss: 0.2376 - val_accuracy: 0.8995\n",
            "Epoch 112/200\n",
            "1536/1536 [==============================] - 3322s 2s/step - loss: 0.2298 - accuracy: 0.9012 - val_loss: 0.2308 - val_accuracy: 0.9014\n",
            "Epoch 113/200\n",
            "1536/1536 [==============================] - 3322s 2s/step - loss: 0.2302 - accuracy: 0.9008 - val_loss: 0.2308 - val_accuracy: 0.9015\n",
            "Epoch 114/200\n",
            "1536/1536 [==============================] - 3325s 2s/step - loss: 0.2306 - accuracy: 0.9010 - val_loss: 0.2411 - val_accuracy: 0.8981\n",
            "Epoch 115/200\n",
            "1536/1536 [==============================] - 3324s 2s/step - loss: 0.2284 - accuracy: 0.9019 - val_loss: 0.2407 - val_accuracy: 0.8976\n",
            "Epoch 116/200\n",
            "1536/1536 [==============================] - 3321s 2s/step - loss: 0.2310 - accuracy: 0.9005 - val_loss: 0.2364 - val_accuracy: 0.8990\n",
            "Epoch 117/200\n",
            "1536/1536 [==============================] - 3327s 2s/step - loss: 0.2295 - accuracy: 0.9012 - val_loss: 0.2313 - val_accuracy: 0.9005\n",
            "Epoch 118/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2284 - accuracy: 0.9018 - val_loss: 0.2323 - val_accuracy: 0.9010\n",
            "Epoch 119/200\n",
            "1536/1536 [==============================] - 3327s 2s/step - loss: 0.2290 - accuracy: 0.9018 - val_loss: 0.2428 - val_accuracy: 0.8976\n",
            "Epoch 120/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2287 - accuracy: 0.9014 - val_loss: 0.2324 - val_accuracy: 0.9003\n",
            "Epoch 121/200\n",
            "1536/1536 [==============================] - 3328s 2s/step - loss: 0.2285 - accuracy: 0.9016 - val_loss: 0.2335 - val_accuracy: 0.9000\n",
            "Epoch 122/200\n",
            "1536/1536 [==============================] - 3327s 2s/step - loss: 0.2291 - accuracy: 0.9017 - val_loss: 0.2354 - val_accuracy: 0.8994\n",
            "Epoch 123/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2293 - accuracy: 0.9015 - val_loss: 0.2308 - val_accuracy: 0.9017\n",
            "Epoch 124/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2261 - accuracy: 0.9028 - val_loss: 0.2328 - val_accuracy: 0.9001\n",
            "Epoch 125/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2287 - accuracy: 0.9018 - val_loss: 0.2314 - val_accuracy: 0.9012\n",
            "Epoch 126/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2288 - accuracy: 0.9016 - val_loss: 0.2304 - val_accuracy: 0.9011\n",
            "Epoch 127/200\n",
            "1536/1536 [==============================] - 3326s 2s/step - loss: 0.2267 - accuracy: 0.9023 - val_loss: 0.2322 - val_accuracy: 0.9001\n",
            "Epoch 128/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2261 - accuracy: 0.9027 - val_loss: 0.2307 - val_accuracy: 0.9005\n",
            "Epoch 129/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2312 - accuracy: 0.9008 - val_loss: 0.2295 - val_accuracy: 0.9008\n",
            "Epoch 130/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2244 - accuracy: 0.9033 - val_loss: 0.2306 - val_accuracy: 0.9011\n",
            "Epoch 131/200\n",
            "1536/1536 [==============================] - 3336s 2s/step - loss: 0.2272 - accuracy: 0.9024 - val_loss: 0.2368 - val_accuracy: 0.8989\n",
            "Epoch 132/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2261 - accuracy: 0.9029 - val_loss: 0.2297 - val_accuracy: 0.9022\n",
            "Epoch 133/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2294 - accuracy: 0.9015 - val_loss: 0.2297 - val_accuracy: 0.9012\n",
            "Epoch 134/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2228 - accuracy: 0.9038 - val_loss: 0.2380 - val_accuracy: 0.8985\n",
            "Epoch 135/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2250 - accuracy: 0.9029 - val_loss: 0.2304 - val_accuracy: 0.9005\n",
            "Epoch 136/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2259 - accuracy: 0.9025 - val_loss: 0.2342 - val_accuracy: 0.9001\n",
            "Epoch 137/200\n",
            "1536/1536 [==============================] - 3329s 2s/step - loss: 0.2251 - accuracy: 0.9030 - val_loss: 0.2318 - val_accuracy: 0.9007\n",
            "Epoch 138/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2266 - accuracy: 0.9025 - val_loss: 0.2294 - val_accuracy: 0.9005\n",
            "Epoch 139/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2241 - accuracy: 0.9036 - val_loss: 0.2352 - val_accuracy: 0.9004\n",
            "Epoch 140/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2245 - accuracy: 0.9030 - val_loss: 0.2349 - val_accuracy: 0.8993\n",
            "Epoch 141/200\n",
            "1536/1536 [==============================] - 3329s 2s/step - loss: 0.2261 - accuracy: 0.9027 - val_loss: 0.2328 - val_accuracy: 0.8998\n",
            "Epoch 142/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2259 - accuracy: 0.9027 - val_loss: 0.2317 - val_accuracy: 0.9007\n",
            "Epoch 143/200\n",
            "1536/1536 [==============================] - 3340s 2s/step - loss: 0.2230 - accuracy: 0.9038 - val_loss: 0.2310 - val_accuracy: 0.9017\n",
            "Epoch 144/200\n",
            "1536/1536 [==============================] - 3336s 2s/step - loss: 0.2250 - accuracy: 0.9030 - val_loss: 0.2316 - val_accuracy: 0.9013\n",
            "Epoch 145/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2254 - accuracy: 0.9030 - val_loss: 0.2382 - val_accuracy: 0.8994\n",
            "Epoch 146/200\n",
            "1536/1536 [==============================] - 3329s 2s/step - loss: 0.2232 - accuracy: 0.9034 - val_loss: 0.2310 - val_accuracy: 0.9007\n",
            "Epoch 147/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2234 - accuracy: 0.9036 - val_loss: 0.2308 - val_accuracy: 0.9009\n",
            "Epoch 148/200\n",
            "1536/1536 [==============================] - 3329s 2s/step - loss: 0.2238 - accuracy: 0.9036 - val_loss: 0.2305 - val_accuracy: 0.9005\n",
            "Epoch 149/200\n",
            "1536/1536 [==============================] - 3329s 2s/step - loss: 0.2247 - accuracy: 0.9034 - val_loss: 0.2352 - val_accuracy: 0.8997\n",
            "Epoch 150/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2235 - accuracy: 0.9036 - val_loss: 0.2328 - val_accuracy: 0.9004\n",
            "Epoch 151/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2226 - accuracy: 0.9039 - val_loss: 0.2353 - val_accuracy: 0.9000\n",
            "Epoch 152/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2229 - accuracy: 0.9039 - val_loss: 0.2319 - val_accuracy: 0.9018\n",
            "Epoch 153/200\n",
            "1536/1536 [==============================] - 3322s 2s/step - loss: 0.2255 - accuracy: 0.9032 - val_loss: 0.2315 - val_accuracy: 0.9008\n",
            "Epoch 154/200\n",
            "1536/1536 [==============================] - 3323s 2s/step - loss: 0.2226 - accuracy: 0.9041 - val_loss: 0.2345 - val_accuracy: 0.8997\n",
            "Epoch 155/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2233 - accuracy: 0.9036 - val_loss: 0.2345 - val_accuracy: 0.8998\n",
            "Epoch 156/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2256 - accuracy: 0.9029 - val_loss: 0.2346 - val_accuracy: 0.9000\n",
            "Epoch 157/200\n",
            "1536/1536 [==============================] - 3332s 2s/step - loss: 0.2207 - accuracy: 0.9046 - val_loss: 0.2305 - val_accuracy: 0.9018\n",
            "Epoch 158/200\n",
            "1536/1536 [==============================] - 3328s 2s/step - loss: 0.2209 - accuracy: 0.9044 - val_loss: 0.2322 - val_accuracy: 0.9011\n",
            "Epoch 159/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2227 - accuracy: 0.9038 - val_loss: 0.2301 - val_accuracy: 0.9021\n",
            "Epoch 160/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2233 - accuracy: 0.9038 - val_loss: 0.2410 - val_accuracy: 0.8968\n",
            "Epoch 161/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2221 - accuracy: 0.9042 - val_loss: 0.2289 - val_accuracy: 0.9021\n",
            "Epoch 162/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2219 - accuracy: 0.9042 - val_loss: 0.2306 - val_accuracy: 0.9020\n",
            "Epoch 163/200\n",
            "1536/1536 [==============================] - 3332s 2s/step - loss: 0.2215 - accuracy: 0.9040 - val_loss: 0.2372 - val_accuracy: 0.8994\n",
            "Epoch 164/200\n",
            "1536/1536 [==============================] - 3328s 2s/step - loss: 0.2219 - accuracy: 0.9043 - val_loss: 0.2398 - val_accuracy: 0.8990\n",
            "Epoch 165/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2212 - accuracy: 0.9046 - val_loss: 0.2562 - val_accuracy: 0.8913\n",
            "Epoch 166/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2218 - accuracy: 0.9044 - val_loss: 0.2340 - val_accuracy: 0.9008\n",
            "Epoch 167/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2202 - accuracy: 0.9050 - val_loss: 0.2339 - val_accuracy: 0.9006\n",
            "Epoch 168/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2221 - accuracy: 0.9041 - val_loss: 0.2347 - val_accuracy: 0.9013\n",
            "Epoch 169/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2192 - accuracy: 0.9051 - val_loss: 0.2397 - val_accuracy: 0.8985\n",
            "Epoch 170/200\n",
            "1536/1536 [==============================] - 3337s 2s/step - loss: 0.2353 - accuracy: 0.9006 - val_loss: 0.2313 - val_accuracy: 0.9020\n",
            "Epoch 171/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2101 - accuracy: 0.9086 - val_loss: 0.2303 - val_accuracy: 0.9023\n",
            "Epoch 172/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2122 - accuracy: 0.9079 - val_loss: 0.2371 - val_accuracy: 0.9002\n",
            "Epoch 173/200\n",
            "1536/1536 [==============================] - 3332s 2s/step - loss: 0.2172 - accuracy: 0.9061 - val_loss: 0.2315 - val_accuracy: 0.9015\n",
            "Epoch 174/200\n",
            "1536/1536 [==============================] - 3329s 2s/step - loss: 0.2206 - accuracy: 0.9046 - val_loss: 0.2392 - val_accuracy: 0.8998\n",
            "Epoch 175/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2191 - accuracy: 0.9056 - val_loss: 0.2400 - val_accuracy: 0.8992\n",
            "Epoch 176/200\n",
            "1536/1536 [==============================] - 3332s 2s/step - loss: 0.2193 - accuracy: 0.9051 - val_loss: 0.2384 - val_accuracy: 0.8994\n",
            "Epoch 177/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2193 - accuracy: 0.9054 - val_loss: 0.2380 - val_accuracy: 0.8998\n",
            "Epoch 178/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2193 - accuracy: 0.9056 - val_loss: 0.2412 - val_accuracy: 0.8976\n",
            "Epoch 179/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2201 - accuracy: 0.9048 - val_loss: 0.2419 - val_accuracy: 0.8980\n",
            "Epoch 180/200\n",
            "1536/1536 [==============================] - 3326s 2s/step - loss: 0.2189 - accuracy: 0.9053 - val_loss: 0.2460 - val_accuracy: 0.8967\n",
            "Epoch 181/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2190 - accuracy: 0.9052 - val_loss: 0.2471 - val_accuracy: 0.8959\n",
            "Epoch 182/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2200 - accuracy: 0.9049 - val_loss: 0.2482 - val_accuracy: 0.8959\n",
            "Epoch 183/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2174 - accuracy: 0.9060 - val_loss: 0.2398 - val_accuracy: 0.8984\n",
            "Epoch 184/200\n",
            "1536/1536 [==============================] - 3327s 2s/step - loss: 0.2187 - accuracy: 0.9053 - val_loss: 0.2395 - val_accuracy: 0.8993\n",
            "Epoch 185/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2193 - accuracy: 0.9051 - val_loss: 0.2327 - val_accuracy: 0.9017\n",
            "Epoch 186/200\n",
            "1536/1536 [==============================] - 3333s 2s/step - loss: 0.2191 - accuracy: 0.9054 - val_loss: 0.2387 - val_accuracy: 0.8996\n",
            "Epoch 187/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2180 - accuracy: 0.9056 - val_loss: 0.2350 - val_accuracy: 0.9012\n",
            "Epoch 188/200\n",
            "1536/1536 [==============================] - 3332s 2s/step - loss: 0.2191 - accuracy: 0.9051 - val_loss: 0.2501 - val_accuracy: 0.8967\n",
            "Epoch 189/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2217 - accuracy: 0.9047 - val_loss: 0.2352 - val_accuracy: 0.8998\n",
            "Epoch 190/200\n",
            "1536/1536 [==============================] - 3332s 2s/step - loss: 0.2161 - accuracy: 0.9067 - val_loss: 0.2355 - val_accuracy: 0.9003\n",
            "Epoch 191/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2178 - accuracy: 0.9062 - val_loss: 0.2315 - val_accuracy: 0.9012\n",
            "Epoch 192/200\n",
            "1536/1536 [==============================] - 3331s 2s/step - loss: 0.2182 - accuracy: 0.9057 - val_loss: 0.2328 - val_accuracy: 0.9007\n",
            "Epoch 193/200\n",
            "1536/1536 [==============================] - 3328s 2s/step - loss: 0.2173 - accuracy: 0.9061 - val_loss: 0.2354 - val_accuracy: 0.9003\n",
            "Epoch 194/200\n",
            "1536/1536 [==============================] - 3330s 2s/step - loss: 0.2177 - accuracy: 0.9059 - val_loss: 0.2321 - val_accuracy: 0.9015\n",
            "Epoch 195/200\n",
            "1536/1536 [==============================] - 3334s 2s/step - loss: 0.2186 - accuracy: 0.9055 - val_loss: 0.2413 - val_accuracy: 0.8989\n",
            "Epoch 196/200\n",
            "1536/1536 [==============================] - 3335s 2s/step - loss: 0.2171 - accuracy: 0.9060 - val_loss: 0.2329 - val_accuracy: 0.9010\n",
            "Epoch 197/200\n",
            "1536/1536 [==============================] - 3341s 2s/step - loss: 0.2173 - accuracy: 0.9060 - val_loss: 0.2426 - val_accuracy: 0.8968\n",
            "Epoch 198/200\n",
            "1536/1536 [==============================] - 3343s 2s/step - loss: 0.2191 - accuracy: 0.9054 - val_loss: 0.2376 - val_accuracy: 0.8993\n",
            "Epoch 199/200\n",
            "1536/1536 [==============================] - 3350s 2s/step - loss: 0.2170 - accuracy: 0.9059 - val_loss: 0.2331 - val_accuracy: 0.9016\n",
            "Epoch 200/200\n",
            "1536/1536 [==============================] - 3342s 2s/step - loss: 0.2172 - accuracy: 0.9062 - val_loss: 0.2356 - val_accuracy: 0.9014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMLConvNet\n",
        "\n",
        "Generate RMLConvNet as defined in O'Shea et Al., Convolutional radio modulation recognition networks, 2016 The implementation is an adaptation of https://github.com/radioML/examples/blob/master/modulation_recognition/RML2016.10a_VTCNN2_example.ipynb"
      ],
      "metadata": {
        "id": "V5QTCvLUFnyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_network = \"RMLConvNet\"\n",
        "\n",
        "model = getattr(neural_nets_keras,'get_{}'.format(name_network))(input_shp,output_shp)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaYaQUpYqWaT",
        "outputId": "3a42a7a7-de9f-4352-df41-184c765b2453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 1024, 2, 1)        0         \n",
            "                                                                 \n",
            " zero_padding2d (ZeroPadding  (None, 1028, 2, 1)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 1026, 2, 256)      1024      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1026, 2, 256)      0         \n",
            "                                                                 \n",
            " zero_padding2d_1 (ZeroPaddi  (None, 1030, 2, 256)     0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 1028, 1, 80)       122960    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1028, 1, 80)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 82240)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               21053696  \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                6168      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21,183,848\n",
            "Trainable params: 21,183,848\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "train_model(model, name_network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HsnhU2AqWu1",
        "outputId": "7f252d05-6c35-47c3-824b-eb2f5948ce51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1536/1536 [==============================] - 7666s 5s/step - loss: 2.3606 - accuracy: 0.1988 - val_loss: 1.3195 - val_accuracy: 0.4618\n",
            "Epoch 2/200\n",
            "1536/1536 [==============================] - 7689s 5s/step - loss: 1.3619 - accuracy: 0.4468 - val_loss: 1.1303 - val_accuracy: 0.5346\n",
            "Epoch 3/200\n",
            "1536/1536 [==============================] - 7715s 5s/step - loss: 1.2649 - accuracy: 0.4821 - val_loss: 1.1024 - val_accuracy: 0.5400\n",
            "Epoch 4/200\n",
            "1536/1536 [==============================] - 7709s 5s/step - loss: 1.2308 - accuracy: 0.4930 - val_loss: 1.1070 - val_accuracy: 0.5368\n",
            "Epoch 5/200\n",
            "1536/1536 [==============================] - 7694s 5s/step - loss: 1.2083 - accuracy: 0.5010 - val_loss: 1.0756 - val_accuracy: 0.5457\n",
            "Epoch 6/200\n",
            "1536/1536 [==============================] - 7712s 5s/step - loss: 1.1932 - accuracy: 0.5067 - val_loss: 1.0658 - val_accuracy: 0.5562\n",
            "Epoch 7/200\n",
            "1536/1536 [==============================] - 7727s 5s/step - loss: 1.1788 - accuracy: 0.5128 - val_loss: 1.0583 - val_accuracy: 0.5608\n",
            "Epoch 8/200\n",
            "1536/1536 [==============================] - 7727s 5s/step - loss: 1.1639 - accuracy: 0.5187 - val_loss: 1.0357 - val_accuracy: 0.5702\n",
            "Epoch 9/200\n",
            "1536/1536 [==============================] - 7730s 5s/step - loss: 1.1510 - accuracy: 0.5240 - val_loss: 1.0399 - val_accuracy: 0.5662\n",
            "Epoch 10/200\n",
            "1536/1536 [==============================] - 7730s 5s/step - loss: 1.1373 - accuracy: 0.5292 - val_loss: 1.0146 - val_accuracy: 0.5779\n",
            "Epoch 11/200\n",
            "1536/1536 [==============================] - 7733s 5s/step - loss: 1.1228 - accuracy: 0.5356 - val_loss: 0.9935 - val_accuracy: 0.5860\n",
            "Epoch 12/200\n",
            "1536/1536 [==============================] - 7734s 5s/step - loss: 1.1098 - accuracy: 0.5400 - val_loss: 0.9943 - val_accuracy: 0.5854\n",
            "Epoch 13/200\n",
            "1536/1536 [==============================] - 7727s 5s/step - loss: 1.0998 - accuracy: 0.5441 - val_loss: 0.9995 - val_accuracy: 0.5779\n",
            "Epoch 14/200\n",
            "1536/1536 [==============================] - 7727s 5s/step - loss: 1.0897 - accuracy: 0.5473 - val_loss: 0.9840 - val_accuracy: 0.5848\n",
            "Epoch 15/200\n",
            "1536/1536 [==============================] - 7743s 5s/step - loss: 1.0831 - accuracy: 0.5500 - val_loss: 0.9930 - val_accuracy: 0.5773\n",
            "Epoch 16/200\n",
            "1536/1536 [==============================] - 7738s 5s/step - loss: 1.0756 - accuracy: 0.5538 - val_loss: 0.9801 - val_accuracy: 0.5830\n",
            "Epoch 17/200\n",
            "1536/1536 [==============================] - 7732s 5s/step - loss: 1.0685 - accuracy: 0.5555 - val_loss: 0.9686 - val_accuracy: 0.5927\n",
            "Epoch 18/200\n",
            "1536/1536 [==============================] - 7735s 5s/step - loss: 1.0637 - accuracy: 0.5573 - val_loss: 0.9682 - val_accuracy: 0.5906\n",
            "Epoch 19/200\n",
            "1536/1536 [==============================] - 7739s 5s/step - loss: 1.0588 - accuracy: 0.5598 - val_loss: 0.9584 - val_accuracy: 0.5953\n",
            "Epoch 20/200\n",
            "1536/1536 [==============================] - 7748s 5s/step - loss: 1.0567 - accuracy: 0.5602 - val_loss: 0.9621 - val_accuracy: 0.5941\n",
            "Epoch 21/200\n",
            "1536/1536 [==============================] - 7746s 5s/step - loss: 1.0506 - accuracy: 0.5630 - val_loss: 0.9481 - val_accuracy: 0.6016\n",
            "Epoch 22/200\n",
            "1536/1536 [==============================] - 7752s 5s/step - loss: 1.0438 - accuracy: 0.5656 - val_loss: 0.9541 - val_accuracy: 0.5928\n",
            "Epoch 23/200\n",
            "1536/1536 [==============================] - 7773s 5s/step - loss: 1.0412 - accuracy: 0.5669 - val_loss: 0.9678 - val_accuracy: 0.5885\n",
            "Epoch 24/200\n",
            "1536/1536 [==============================] - 7772s 5s/step - loss: 1.0344 - accuracy: 0.5697 - val_loss: 0.9255 - val_accuracy: 0.6054\n",
            "Epoch 25/200\n",
            "1536/1536 [==============================] - 7763s 5s/step - loss: 1.0321 - accuracy: 0.5703 - val_loss: 0.9418 - val_accuracy: 0.6014\n",
            "Epoch 26/200\n",
            "1536/1536 [==============================] - 7760s 5s/step - loss: 1.0290 - accuracy: 0.5715 - val_loss: 0.9476 - val_accuracy: 0.5931\n",
            "Epoch 27/200\n",
            "1536/1536 [==============================] - 7755s 5s/step - loss: 1.0223 - accuracy: 0.5741 - val_loss: 0.9797 - val_accuracy: 0.5774\n",
            "Epoch 28/200\n",
            "1536/1536 [==============================] - 7758s 5s/step - loss: 1.0145 - accuracy: 0.5770 - val_loss: 0.9193 - val_accuracy: 0.6145\n",
            "Epoch 29/200\n",
            "1536/1536 [==============================] - 7759s 5s/step - loss: 1.0099 - accuracy: 0.5782 - val_loss: 0.8935 - val_accuracy: 0.6252\n",
            "Epoch 30/200\n",
            "1536/1536 [==============================] - 7757s 5s/step - loss: 1.0087 - accuracy: 0.5787 - val_loss: 0.8925 - val_accuracy: 0.6206\n",
            "Epoch 31/200\n",
            "1536/1536 [==============================] - 7771s 5s/step - loss: 1.0015 - accuracy: 0.5815 - val_loss: 0.8980 - val_accuracy: 0.6149\n",
            "Epoch 32/200\n",
            "1536/1536 [==============================] - 7766s 5s/step - loss: 0.9976 - accuracy: 0.5837 - val_loss: 0.8887 - val_accuracy: 0.6218\n",
            "Epoch 33/200\n",
            "1536/1536 [==============================] - 7767s 5s/step - loss: 0.9978 - accuracy: 0.5821 - val_loss: 0.8965 - val_accuracy: 0.6196\n",
            "Epoch 34/200\n",
            "1536/1536 [==============================] - 7767s 5s/step - loss: 0.9905 - accuracy: 0.5855 - val_loss: 0.9107 - val_accuracy: 0.6099\n",
            "Epoch 35/200\n",
            "1536/1536 [==============================] - 7769s 5s/step - loss: 0.9888 - accuracy: 0.5865 - val_loss: 0.8980 - val_accuracy: 0.6147\n",
            "Epoch 36/200\n",
            "1536/1536 [==============================] - 7767s 5s/step - loss: 0.9877 - accuracy: 0.5868 - val_loss: 0.8925 - val_accuracy: 0.6153\n",
            "Epoch 37/200\n",
            "1536/1536 [==============================] - 7768s 5s/step - loss: 0.9841 - accuracy: 0.5884 - val_loss: 0.8802 - val_accuracy: 0.6237\n",
            "Epoch 38/200\n",
            "1536/1536 [==============================] - 7768s 5s/step - loss: 0.9809 - accuracy: 0.5898 - val_loss: 0.8799 - val_accuracy: 0.6234\n",
            "Epoch 39/200\n",
            "1536/1536 [==============================] - 8079s 5s/step - loss: 0.9804 - accuracy: 0.5888 - val_loss: 0.8985 - val_accuracy: 0.6145\n",
            "Epoch 40/200\n",
            "1536/1536 [==============================] - 8085s 5s/step - loss: 0.9764 - accuracy: 0.5903 - val_loss: 0.8699 - val_accuracy: 0.6304\n",
            "Epoch 41/200\n",
            "1536/1536 [==============================] - 8089s 5s/step - loss: 0.9758 - accuracy: 0.5914 - val_loss: 0.8848 - val_accuracy: 0.6201\n",
            "Epoch 42/200\n",
            "1536/1536 [==============================] - 8083s 5s/step - loss: 0.9710 - accuracy: 0.5934 - val_loss: 0.8603 - val_accuracy: 0.6333\n",
            "Epoch 43/200\n",
            "1536/1536 [==============================] - 8077s 5s/step - loss: 0.9711 - accuracy: 0.5929 - val_loss: 0.8727 - val_accuracy: 0.6261\n",
            "Epoch 44/200\n",
            "1536/1536 [==============================] - 8061s 5s/step - loss: 0.9685 - accuracy: 0.5934 - val_loss: 0.8620 - val_accuracy: 0.6308\n",
            "Epoch 45/200\n",
            "1536/1536 [==============================] - 8064s 5s/step - loss: 0.9674 - accuracy: 0.5937 - val_loss: 0.8702 - val_accuracy: 0.6282\n",
            "Epoch 46/200\n",
            "1536/1536 [==============================] - 8064s 5s/step - loss: 0.9649 - accuracy: 0.5952 - val_loss: 0.8794 - val_accuracy: 0.6224\n",
            "Epoch 47/200\n",
            "1536/1536 [==============================] - 8071s 5s/step - loss: 0.9625 - accuracy: 0.5965 - val_loss: 0.9125 - val_accuracy: 0.6049\n",
            "Epoch 48/200\n",
            "1536/1536 [==============================] - 8079s 5s/step - loss: 0.9611 - accuracy: 0.5968 - val_loss: 0.8726 - val_accuracy: 0.6314\n",
            "Epoch 49/200\n",
            "1536/1536 [==============================] - 8077s 5s/step - loss: 0.9594 - accuracy: 0.5968 - val_loss: 0.8652 - val_accuracy: 0.6261\n",
            "Epoch 50/200\n",
            "1536/1536 [==============================] - 8077s 5s/step - loss: 0.9577 - accuracy: 0.5978 - val_loss: 0.8675 - val_accuracy: 0.6267\n",
            "Epoch 51/200\n",
            "1536/1536 [==============================] - 8071s 5s/step - loss: 0.9561 - accuracy: 0.5986 - val_loss: 0.8526 - val_accuracy: 0.6369\n",
            "Epoch 52/200\n",
            "1536/1536 [==============================] - 8085s 5s/step - loss: 0.9544 - accuracy: 0.5990 - val_loss: 0.8682 - val_accuracy: 0.6292\n",
            "Epoch 53/200\n",
            "1536/1536 [==============================] - 8086s 5s/step - loss: 0.9522 - accuracy: 0.5996 - val_loss: 0.8805 - val_accuracy: 0.6201\n",
            "Epoch 54/200\n",
            "1536/1536 [==============================] - 8092s 5s/step - loss: 0.9508 - accuracy: 0.6006 - val_loss: 0.8747 - val_accuracy: 0.6251\n",
            "Epoch 55/200\n",
            "1536/1536 [==============================] - 8100s 5s/step - loss: 0.9510 - accuracy: 0.6002 - val_loss: 0.8718 - val_accuracy: 0.6244\n",
            "Epoch 56/200\n",
            "1536/1536 [==============================] - 8098s 5s/step - loss: 0.9463 - accuracy: 0.6027 - val_loss: 0.8492 - val_accuracy: 0.6399\n",
            "Epoch 57/200\n",
            "1536/1536 [==============================] - 8100s 5s/step - loss: 0.9428 - accuracy: 0.6041 - val_loss: 0.8664 - val_accuracy: 0.6282\n",
            "Epoch 58/200\n",
            "1536/1536 [==============================] - 8092s 5s/step - loss: 0.9381 - accuracy: 0.6053 - val_loss: 0.8705 - val_accuracy: 0.6271\n",
            "Epoch 59/200\n",
            "1536/1536 [==============================] - 8100s 5s/step - loss: 0.9364 - accuracy: 0.6063 - val_loss: 0.8413 - val_accuracy: 0.6377\n",
            "Epoch 60/200\n",
            "1536/1536 [==============================] - 8091s 5s/step - loss: 0.9309 - accuracy: 0.6082 - val_loss: 0.8585 - val_accuracy: 0.6275\n",
            "Epoch 61/200\n",
            "1536/1536 [==============================] - 8093s 5s/step - loss: 0.9277 - accuracy: 0.6107 - val_loss: 0.8397 - val_accuracy: 0.6334\n",
            "Epoch 62/200\n",
            "1536/1536 [==============================] - 8093s 5s/step - loss: 0.9234 - accuracy: 0.6110 - val_loss: 0.8436 - val_accuracy: 0.6364\n",
            "Epoch 63/200\n",
            "1536/1536 [==============================] - 8082s 5s/step - loss: 0.9233 - accuracy: 0.6120 - val_loss: 0.8256 - val_accuracy: 0.6446\n",
            "Epoch 64/200\n",
            "1536/1536 [==============================] - 8085s 5s/step - loss: 0.9212 - accuracy: 0.6131 - val_loss: 0.8387 - val_accuracy: 0.6379\n",
            "Epoch 65/200\n",
            "1536/1536 [==============================] - 8082s 5s/step - loss: 0.9196 - accuracy: 0.6139 - val_loss: 0.8302 - val_accuracy: 0.6458\n",
            "Epoch 66/200\n",
            "1536/1536 [==============================] - 8074s 5s/step - loss: 0.9173 - accuracy: 0.6143 - val_loss: 0.8220 - val_accuracy: 0.6431\n",
            "Epoch 67/200\n",
            "1536/1536 [==============================] - 8080s 5s/step - loss: 0.9167 - accuracy: 0.6144 - val_loss: 0.8358 - val_accuracy: 0.6429\n",
            "Epoch 68/200\n",
            "1536/1536 [==============================] - 8084s 5s/step - loss: 0.9168 - accuracy: 0.6140 - val_loss: 0.8216 - val_accuracy: 0.6494\n",
            "Epoch 69/200\n",
            "1536/1536 [==============================] - 8076s 5s/step - loss: 0.9134 - accuracy: 0.6161 - val_loss: 0.8291 - val_accuracy: 0.6480\n",
            "Epoch 70/200\n",
            "1536/1536 [==============================] - 8086s 5s/step - loss: 0.9127 - accuracy: 0.6158 - val_loss: 0.8208 - val_accuracy: 0.6453\n",
            "Epoch 71/200\n",
            "1536/1536 [==============================] - 8206s 5s/step - loss: 0.9120 - accuracy: 0.6160 - val_loss: 0.8172 - val_accuracy: 0.6478\n",
            "Epoch 72/200\n",
            "1536/1536 [==============================] - 8321s 5s/step - loss: 0.9092 - accuracy: 0.6173 - val_loss: 0.8157 - val_accuracy: 0.6494\n",
            "Epoch 73/200\n",
            "1536/1536 [==============================] - 8326s 5s/step - loss: 0.9084 - accuracy: 0.6174 - val_loss: 0.8238 - val_accuracy: 0.6442\n",
            "Epoch 74/200\n",
            "1536/1536 [==============================] - 8320s 5s/step - loss: 0.9066 - accuracy: 0.6184 - val_loss: 0.8143 - val_accuracy: 0.6472\n",
            "Epoch 75/200\n",
            "1536/1536 [==============================] - 8312s 5s/step - loss: 0.9072 - accuracy: 0.6183 - val_loss: 0.8217 - val_accuracy: 0.6501\n",
            "Epoch 76/200\n",
            "1536/1536 [==============================] - 8289s 5s/step - loss: 0.9061 - accuracy: 0.6192 - val_loss: 0.8154 - val_accuracy: 0.6539\n",
            "Epoch 77/200\n",
            "1536/1536 [==============================] - 8291s 5s/step - loss: 0.9033 - accuracy: 0.6197 - val_loss: 0.8237 - val_accuracy: 0.6490\n",
            "Epoch 78/200\n",
            "1536/1536 [==============================] - 8304s 5s/step - loss: 0.9024 - accuracy: 0.6199 - val_loss: 0.8167 - val_accuracy: 0.6517\n",
            "Epoch 79/200\n",
            "1536/1536 [==============================] - 8288s 5s/step - loss: 0.9028 - accuracy: 0.6206 - val_loss: 0.8281 - val_accuracy: 0.6416\n",
            "Epoch 80/200\n",
            "1536/1536 [==============================] - 8294s 5s/step - loss: 0.9010 - accuracy: 0.6205 - val_loss: 0.8327 - val_accuracy: 0.6433\n",
            "Epoch 81/200\n",
            "1536/1536 [==============================] - 8286s 5s/step - loss: 0.8989 - accuracy: 0.6220 - val_loss: 0.8171 - val_accuracy: 0.6463\n",
            "Epoch 82/200\n",
            "1536/1536 [==============================] - 7985s 5s/step - loss: 0.8982 - accuracy: 0.6228 - val_loss: 0.8194 - val_accuracy: 0.6475\n",
            "Epoch 83/200\n",
            "1536/1536 [==============================] - 7991s 5s/step - loss: 0.8954 - accuracy: 0.6230 - val_loss: 0.8157 - val_accuracy: 0.6464\n",
            "Epoch 84/200\n",
            "1536/1536 [==============================] - 8044s 5s/step - loss: 0.8939 - accuracy: 0.6236 - val_loss: 0.8222 - val_accuracy: 0.6463\n",
            "Epoch 85/200\n",
            "1536/1536 [==============================] - 8082s 5s/step - loss: 0.8931 - accuracy: 0.6243 - val_loss: 0.8011 - val_accuracy: 0.6528\n",
            "Epoch 86/200\n",
            "1536/1536 [==============================] - 7998s 5s/step - loss: 0.8916 - accuracy: 0.6256 - val_loss: 0.8094 - val_accuracy: 0.6545\n",
            "Epoch 87/200\n",
            "1536/1536 [==============================] - 8021s 5s/step - loss: 0.8888 - accuracy: 0.6258 - val_loss: 0.8102 - val_accuracy: 0.6534\n",
            "Epoch 88/200\n",
            "1536/1536 [==============================] - 8069s 5s/step - loss: 0.8902 - accuracy: 0.6257 - val_loss: 0.7928 - val_accuracy: 0.6588\n",
            "Epoch 89/200\n",
            "1536/1536 [==============================] - 8016s 5s/step - loss: 0.8877 - accuracy: 0.6260 - val_loss: 0.8083 - val_accuracy: 0.6506\n",
            "Epoch 90/200\n",
            "1536/1536 [==============================] - 8007s 5s/step - loss: 0.8848 - accuracy: 0.6277 - val_loss: 0.8200 - val_accuracy: 0.6444\n",
            "Epoch 91/200\n",
            "1536/1536 [==============================] - 8009s 5s/step - loss: 0.8838 - accuracy: 0.6283 - val_loss: 0.8094 - val_accuracy: 0.6482\n",
            "Epoch 92/200\n",
            "1536/1536 [==============================] - 8082s 5s/step - loss: 0.8834 - accuracy: 0.6284 - val_loss: 0.8093 - val_accuracy: 0.6507\n",
            "Epoch 93/200\n",
            "1536/1536 [==============================] - 8037s 5s/step - loss: 0.8808 - accuracy: 0.6291 - val_loss: 0.7935 - val_accuracy: 0.6604\n",
            "Epoch 94/200\n",
            "1536/1536 [==============================] - 8028s 5s/step - loss: 0.8816 - accuracy: 0.6292 - val_loss: 0.8029 - val_accuracy: 0.6530\n",
            "Epoch 95/200\n",
            "1536/1536 [==============================] - 8077s 5s/step - loss: 0.8793 - accuracy: 0.6303 - val_loss: 0.8183 - val_accuracy: 0.6433\n",
            "Epoch 96/200\n",
            "1536/1536 [==============================] - 8095s 5s/step - loss: 0.8791 - accuracy: 0.6304 - val_loss: 0.7986 - val_accuracy: 0.6545\n",
            "Epoch 97/200\n",
            "1536/1536 [==============================] - 7996s 5s/step - loss: 0.8778 - accuracy: 0.6306 - val_loss: 0.8130 - val_accuracy: 0.6499\n",
            "Epoch 98/200\n",
            "1536/1536 [==============================] - 8015s 5s/step - loss: 0.8742 - accuracy: 0.6319 - val_loss: 0.7922 - val_accuracy: 0.6581\n",
            "Epoch 99/200\n",
            "1536/1536 [==============================] - 8087s 5s/step - loss: 0.8739 - accuracy: 0.6329 - val_loss: 0.8015 - val_accuracy: 0.6534\n",
            "Epoch 100/200\n",
            "1536/1536 [==============================] - 8039s 5s/step - loss: 0.8716 - accuracy: 0.6334 - val_loss: 0.7976 - val_accuracy: 0.6544\n",
            "Epoch 101/200\n",
            "1536/1536 [==============================] - 8058s 5s/step - loss: 0.8710 - accuracy: 0.6336 - val_loss: 0.8087 - val_accuracy: 0.6507\n",
            "Epoch 102/200\n",
            "1536/1536 [==============================] - 8081s 5s/step - loss: 0.8695 - accuracy: 0.6349 - val_loss: 0.8001 - val_accuracy: 0.6568\n",
            "Epoch 103/200\n",
            "1536/1536 [==============================] - 8098s 5s/step - loss: 0.8701 - accuracy: 0.6339 - val_loss: 0.7805 - val_accuracy: 0.6604\n",
            "Epoch 104/200\n",
            "1536/1536 [==============================] - 8049s 5s/step - loss: 0.8662 - accuracy: 0.6356 - val_loss: 0.8007 - val_accuracy: 0.6504\n",
            "Epoch 105/200\n",
            "1536/1536 [==============================] - 8059s 5s/step - loss: 0.8667 - accuracy: 0.6360 - val_loss: 0.7957 - val_accuracy: 0.6550\n",
            "Epoch 106/200\n",
            "1536/1536 [==============================] - 8073s 5s/step - loss: 0.8669 - accuracy: 0.6358 - val_loss: 0.7970 - val_accuracy: 0.6531\n",
            "Epoch 107/200\n",
            "1536/1536 [==============================] - 8070s 5s/step - loss: 0.8640 - accuracy: 0.6374 - val_loss: 0.7949 - val_accuracy: 0.6578\n",
            "Epoch 108/200\n",
            "1536/1536 [==============================] - 8015s 5s/step - loss: 0.8637 - accuracy: 0.6374 - val_loss: 0.7921 - val_accuracy: 0.6579\n",
            "Epoch 109/200\n",
            "1536/1536 [==============================] - 8038s 5s/step - loss: 0.8639 - accuracy: 0.6377 - val_loss: 0.8168 - val_accuracy: 0.6494\n",
            "Epoch 110/200\n",
            "1536/1536 [==============================] - 8074s 5s/step - loss: 0.8634 - accuracy: 0.6378 - val_loss: 0.7797 - val_accuracy: 0.6635\n",
            "Epoch 111/200\n",
            "1536/1536 [==============================] - 8024s 5s/step - loss: 0.8603 - accuracy: 0.6388 - val_loss: 0.7794 - val_accuracy: 0.6600\n",
            "Epoch 112/200\n",
            "1536/1536 [==============================] - 8035s 5s/step - loss: 0.8594 - accuracy: 0.6397 - val_loss: 0.7941 - val_accuracy: 0.6544\n",
            "Epoch 113/200\n",
            "1536/1536 [==============================] - 8021s 5s/step - loss: 0.8589 - accuracy: 0.6397 - val_loss: 0.7813 - val_accuracy: 0.6608\n",
            "Epoch 114/200\n",
            "1536/1536 [==============================] - 8024s 5s/step - loss: 0.8568 - accuracy: 0.6410 - val_loss: 0.7950 - val_accuracy: 0.6562\n",
            "Epoch 115/200\n",
            "1536/1536 [==============================] - 8019s 5s/step - loss: 0.8562 - accuracy: 0.6411 - val_loss: 0.7863 - val_accuracy: 0.6554\n",
            "Epoch 116/200\n",
            "1536/1536 [==============================] - 8055s 5s/step - loss: 0.8546 - accuracy: 0.6424 - val_loss: 0.7992 - val_accuracy: 0.6560\n",
            "Epoch 117/200\n",
            "1536/1536 [==============================] - 8069s 5s/step - loss: 0.8530 - accuracy: 0.6426 - val_loss: 0.8014 - val_accuracy: 0.6528\n",
            "Epoch 118/200\n",
            "1536/1536 [==============================] - 8018s 5s/step - loss: 0.8532 - accuracy: 0.6435 - val_loss: 0.7841 - val_accuracy: 0.6577\n",
            "Epoch 119/200\n",
            "1536/1536 [==============================] - 8039s 5s/step - loss: 0.8546 - accuracy: 0.6427 - val_loss: 0.8219 - val_accuracy: 0.6462\n",
            "Epoch 120/200\n",
            "1536/1536 [==============================] - 8099s 5s/step - loss: 0.8514 - accuracy: 0.6439 - val_loss: 0.7845 - val_accuracy: 0.6580\n",
            "Epoch 121/200\n",
            "1536/1536 [==============================] - 8022s 5s/step - loss: 0.8501 - accuracy: 0.6445 - val_loss: 0.7825 - val_accuracy: 0.6603\n",
            "Epoch 122/200\n",
            "1536/1536 [==============================] - 8053s 5s/step - loss: 0.8486 - accuracy: 0.6456 - val_loss: 0.7818 - val_accuracy: 0.6600\n",
            "Epoch 123/200\n",
            "1536/1536 [==============================] - 8082s 5s/step - loss: 0.8497 - accuracy: 0.6449 - val_loss: 0.7875 - val_accuracy: 0.6604\n",
            "Epoch 124/200\n",
            "1536/1536 [==============================] - 8055s 5s/step - loss: 0.8504 - accuracy: 0.6447 - val_loss: 0.7934 - val_accuracy: 0.6543\n",
            "Epoch 125/200\n",
            "1536/1536 [==============================] - 8050s 5s/step - loss: 0.8450 - accuracy: 0.6474 - val_loss: 0.7720 - val_accuracy: 0.6680\n",
            "Epoch 126/200\n",
            "1536/1536 [==============================] - 8035s 5s/step - loss: 0.8477 - accuracy: 0.6454 - val_loss: 0.7922 - val_accuracy: 0.6550\n",
            "Epoch 127/200\n",
            "1536/1536 [==============================] - 8042s 5s/step - loss: 0.8458 - accuracy: 0.6473 - val_loss: 0.8016 - val_accuracy: 0.6507\n",
            "Epoch 128/200\n",
            "1536/1536 [==============================] - 8054s 5s/step - loss: 0.8452 - accuracy: 0.6474 - val_loss: 0.7871 - val_accuracy: 0.6611\n",
            "Epoch 129/200\n",
            "1536/1536 [==============================] - 8044s 5s/step - loss: 0.8439 - accuracy: 0.6472 - val_loss: 0.7912 - val_accuracy: 0.6543\n",
            "Epoch 130/200\n",
            "1536/1536 [==============================] - 8039s 5s/step - loss: 0.8437 - accuracy: 0.6487 - val_loss: 0.7812 - val_accuracy: 0.6609\n",
            "Epoch 131/200\n",
            "1536/1536 [==============================] - 8039s 5s/step - loss: 0.8422 - accuracy: 0.6486 - val_loss: 0.7835 - val_accuracy: 0.6655\n",
            "Epoch 132/200\n",
            "1536/1536 [==============================] - 8033s 5s/step - loss: 0.8432 - accuracy: 0.6488 - val_loss: 0.7867 - val_accuracy: 0.6605\n",
            "Epoch 133/200\n",
            "1536/1536 [==============================] - 8040s 5s/step - loss: 0.8393 - accuracy: 0.6502 - val_loss: 0.7749 - val_accuracy: 0.6633\n",
            "Epoch 134/200\n",
            "1536/1536 [==============================] - 8036s 5s/step - loss: 0.8419 - accuracy: 0.6491 - val_loss: 0.7858 - val_accuracy: 0.6572\n",
            "Epoch 135/200\n",
            "1536/1536 [==============================] - 8036s 5s/step - loss: 0.8395 - accuracy: 0.6498 - val_loss: 0.7809 - val_accuracy: 0.6604\n",
            "Epoch 136/200\n",
            "1536/1536 [==============================] - 8073s 5s/step - loss: 0.8395 - accuracy: 0.6501 - val_loss: 0.7810 - val_accuracy: 0.6592\n",
            "Epoch 137/200\n",
            "1536/1536 [==============================] - 8070s 5s/step - loss: 0.8367 - accuracy: 0.6519 - val_loss: 0.7817 - val_accuracy: 0.6611\n",
            "Epoch 138/200\n",
            "1536/1536 [==============================] - 8060s 5s/step - loss: 0.8367 - accuracy: 0.6514 - val_loss: 0.7734 - val_accuracy: 0.6633\n",
            "Epoch 139/200\n",
            "1536/1536 [==============================] - 8120s 5s/step - loss: 0.8365 - accuracy: 0.6517 - val_loss: 0.7791 - val_accuracy: 0.6597\n",
            "Epoch 140/200\n",
            "1536/1536 [==============================] - 8134s 5s/step - loss: 0.8352 - accuracy: 0.6524 - val_loss: 0.7846 - val_accuracy: 0.6574\n",
            "Epoch 141/200\n",
            "1536/1536 [==============================] - 8134s 5s/step - loss: 0.8331 - accuracy: 0.6532 - val_loss: 0.7784 - val_accuracy: 0.6569\n",
            "Epoch 142/200\n",
            "1536/1536 [==============================] - 8127s 5s/step - loss: 0.8323 - accuracy: 0.6535 - val_loss: 0.7847 - val_accuracy: 0.6590\n",
            "Epoch 143/200\n",
            "1536/1536 [==============================] - 8131s 5s/step - loss: 0.8307 - accuracy: 0.6543 - val_loss: 0.7804 - val_accuracy: 0.6552\n",
            "Epoch 144/200\n",
            "1536/1536 [==============================] - 8127s 5s/step - loss: 0.8310 - accuracy: 0.6541 - val_loss: 0.7873 - val_accuracy: 0.6575\n",
            "Epoch 145/200\n",
            "1536/1536 [==============================] - 8122s 5s/step - loss: 0.8315 - accuracy: 0.6542 - val_loss: 0.8094 - val_accuracy: 0.6476\n",
            "Epoch 146/200\n",
            "1536/1536 [==============================] - 8129s 5s/step - loss: 0.8292 - accuracy: 0.6552 - val_loss: 0.7873 - val_accuracy: 0.6574\n",
            "Epoch 147/200\n",
            "1536/1536 [==============================] - 8125s 5s/step - loss: 0.8280 - accuracy: 0.6556 - val_loss: 0.7923 - val_accuracy: 0.6529\n",
            "Epoch 148/200\n",
            "1536/1536 [==============================] - 8120s 5s/step - loss: 0.8269 - accuracy: 0.6562 - val_loss: 0.7682 - val_accuracy: 0.6657\n",
            "Epoch 149/200\n",
            "1536/1536 [==============================] - 8121s 5s/step - loss: 0.8267 - accuracy: 0.6569 - val_loss: 0.7654 - val_accuracy: 0.6677\n",
            "Epoch 150/200\n",
            "1536/1536 [==============================] - 8123s 5s/step - loss: 0.8276 - accuracy: 0.6563 - val_loss: 0.7714 - val_accuracy: 0.6634\n",
            "Epoch 151/200\n",
            "1536/1536 [==============================] - 8117s 5s/step - loss: 0.8232 - accuracy: 0.6577 - val_loss: 0.7668 - val_accuracy: 0.6664\n",
            "Epoch 152/200\n",
            "1536/1536 [==============================] - 8119s 5s/step - loss: 0.8243 - accuracy: 0.6573 - val_loss: 0.7708 - val_accuracy: 0.6647\n",
            "Epoch 153/200\n",
            "1536/1536 [==============================] - 8119s 5s/step - loss: 0.8242 - accuracy: 0.6582 - val_loss: 0.7757 - val_accuracy: 0.6664\n",
            "Epoch 154/200\n",
            "1536/1536 [==============================] - 8117s 5s/step - loss: 0.8244 - accuracy: 0.6581 - val_loss: 0.7698 - val_accuracy: 0.6657\n",
            "Epoch 155/200\n",
            "1536/1536 [==============================] - 8115s 5s/step - loss: 0.8199 - accuracy: 0.6595 - val_loss: 0.7614 - val_accuracy: 0.6701\n",
            "Epoch 156/200\n",
            "1536/1536 [==============================] - 8121s 5s/step - loss: 0.8190 - accuracy: 0.6602 - val_loss: 0.7607 - val_accuracy: 0.6721\n",
            "Epoch 157/200\n",
            "1536/1536 [==============================] - 8112s 5s/step - loss: 0.8212 - accuracy: 0.6592 - val_loss: 0.7652 - val_accuracy: 0.6688\n",
            "Epoch 158/200\n",
            "1536/1536 [==============================] - 8113s 5s/step - loss: 0.8192 - accuracy: 0.6597 - val_loss: 0.7646 - val_accuracy: 0.6664\n",
            "Epoch 159/200\n",
            "1536/1536 [==============================] - 8120s 5s/step - loss: 0.8189 - accuracy: 0.6601 - val_loss: 0.7614 - val_accuracy: 0.6712\n",
            "Epoch 160/200\n",
            "1536/1536 [==============================] - 8115s 5s/step - loss: 0.8172 - accuracy: 0.6613 - val_loss: 0.7557 - val_accuracy: 0.6737\n",
            "Epoch 161/200\n",
            "1536/1536 [==============================] - 8108s 5s/step - loss: 0.8180 - accuracy: 0.6606 - val_loss: 0.7598 - val_accuracy: 0.6746\n",
            "Epoch 162/200\n",
            "1536/1536 [==============================] - 8123s 5s/step - loss: 0.8173 - accuracy: 0.6613 - val_loss: 0.7725 - val_accuracy: 0.6608\n",
            "Epoch 163/200\n",
            "1536/1536 [==============================] - 8121s 5s/step - loss: 0.8154 - accuracy: 0.6620 - val_loss: 0.7572 - val_accuracy: 0.6704\n",
            "Epoch 164/200\n",
            "1536/1536 [==============================] - 8125s 5s/step - loss: 0.8167 - accuracy: 0.6615 - val_loss: 0.7609 - val_accuracy: 0.6647\n",
            "Epoch 165/200\n",
            " 496/1536 [========>.....................] - ETA: 1:15:08 - loss: 0.8134 - accuracy: 0.6620"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMLCNNVGG\n",
        "\n",
        "Generate RML CNN/VGG as defined in O'Shea et Al., Over-the-Air Deep Learning Based Radio Signal Classification, 2018. The implementation is an adaptation of https://github.com/leena201818/radioml/blob/master/rmlmodels/VGGLikeModel.py"
      ],
      "metadata": {
        "id": "U1ezlD0rqgkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_network = \"RMLCNNVGG\"\n",
        "\n",
        "model = getattr(neural_nets_keras,'get_{}'.format(name_network))(input_shp,output_shp)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "5YSQhZO5q3VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "train_model(model, name_network)"
      ],
      "metadata": {
        "id": "V0kNr5aHq3xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMLResNet\n",
        "\n",
        "Generate RML Residual Network as defined in O'Shea et Al., Over-the-Air Deep Learning Based Radio Signal Classification, 2018. The implementation is an adaptation of https://github.com/liuzhejun/ResNet-for-Radio-Recognition/blob/master/ResNet_Model.ipynb"
      ],
      "metadata": {
        "id": "piO_gbgxrK3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_network = \"RMLResNet\"\n",
        "\n",
        "model = getattr(neural_nets_keras,'get_{}'.format(name_network))(input_shp,output_shp)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gHBpSg5nFxD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "train_model(model, name_network)"
      ],
      "metadata": {
        "id": "p6rBE1qyJ25B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results and comparisons**\n"
      ],
      "metadata": {
        "id": "xJ-9LpkYKEwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from_name_to_legend = {\n",
        "    'RMLConvNet':'RML-ConvNet',\n",
        "    'RMLCNNVGG':'RML-CNN/VGG',\n",
        "    'RMLResNet':'RML-ResNet',\n",
        "    'LModCNN':'Mod-LCNN (new)',\n",
        "    'LModCNNResNetRelu':'Mod-LRCNN (new)',\n",
        "}\n",
        "\n",
        "networks_to_plot = ['RMLConvNet','RMLCNNVGG','RMLResNet','LModCNN','LModCNNResNetRelu' ]"
      ],
      "metadata": {
        "id": "k2zkxXxXKQQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot training trajectories\n",
        "\n",
        "Plots the evolution of error through learning epochs both for train and test dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "R8ftGx8UKaPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "for name_network in networks_to_plot:\n",
        "\n",
        "    data = np.loadtxt(join(log_path,'history-{}-{}-trained{}.txt'.format(dataset_name,name_network,signal_duration)))\n",
        "\n",
        "    p = plt.plot(data[:,0], data[:,1],\n",
        "        label=from_name_to_legend[name_network],linewidth=2)\n",
        "    plt.plot(data[:,0], data[:,2],':',\n",
        "        linewidth=2,color=p[0].get_color(),alpha=0.8)\n",
        "\n",
        "plt.plot(data[:,0] ,1.-1./output_shp*np.ones_like(data[:,0]),\n",
        "    '--',label='Random classifier',linewidth=2,color='black')\n",
        "\n",
        "plt.legend(ncol=2,loc=1)\n",
        "plt.grid()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title(f'Data set: {dataset_name}, Samples per signal: {signal_duration}')\n",
        "plt.ylim([0.,1.])\n",
        "plt.xlim([1,nb_epoch])\n",
        "plt.savefig(join(log_path,'network_comparison_{}-trained{}.pdf'.format(dataset_name,signal_duration)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eHeHerHCKeEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solid curves are for the test set and dotted curves for the training set\n",
        "\n",
        "Lower is better"
      ],
      "metadata": {
        "id": "gqyQq8HOOM7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print final performances\n",
        "\n",
        "Displays test accuracy for different algorithmes and train/test computation time"
      ],
      "metadata": {
        "id": "dfaju-fJOcDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(join(log_path,'perfs-{}-{}-trained{}.txt'.format(dataset_name,'all',signal_duration)), 'w')\n",
        "\n",
        "f.write(f'#Performance evaluations, Data set: {dataset_name}, Samples per signal: {signal_duration}\\n')\n",
        "\n",
        "f.write(f\"#{'':20s}{'Loss':15s}{'Accuracy':15s}{'Training time':20s}{'Inference time':20s}{'# Parameters':15s}\\n\")\n",
        "f.write(f\"#{'':20s}{'':15s}{'':15s}{'(s/epoch)':20s}{'(ms/signal)':20s}\\n\")\n",
        "f.write(f\"#{'-'*110}\\n\")\n",
        "\n",
        "for name_network in networks_to_plot:\n",
        "\n",
        "    training_time_vec=  np.loadtxt(join(log_path,'history_time-{}-{}-trained{}.txt'.format(dataset_name,name_network,signal_duration)))\n",
        "    training_time = training_time_vec[:,1].mean()\n",
        "\n",
        "    print(join(log_path,'model-{}-{}-trained{}.h5'.format(dataset_name,name_network,signal_duration)))\n",
        "\n",
        "    model = load_model(join(log_path,'model-{}-{}-trained{}.h5'.format(dataset_name,name_network,signal_duration)),\n",
        "        compile=True)\n",
        "    mod_size = model.count_params()\n",
        "\n",
        "    t = time.time()\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test,\n",
        "        verbose=False,batch_size=batch_size)\n",
        "    t_proc= 1000.0*(time.time()-t)/X_test.shape[0] #ms\n",
        "\n",
        "    f.write(f\"{from_name_to_legend[name_network]:20s}{test_loss:2.3f}{'':10s}{test_acc:2.3f}{'':10s}{training_time:>4.1f}{'':17s}{t_proc:>4.2}{'':16s}{mod_size:>10,d}\\n\")\n",
        "\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "3QRmL-9bOR0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(join(log_path,'perfs-{}-{}-trained{}.txt'.format(dataset_name,'all',signal_duration)), 'r')\n",
        "print(f.read())\n",
        "f.close()"
      ],
      "metadata": {
        "id": "R9PNaC9SO-zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_session()"
      ],
      "metadata": {
        "id": "p-tRPsXpPPb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}